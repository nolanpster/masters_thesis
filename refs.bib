@book{baier2008principles,
  title={Principles of model checking},
  author={Baier, Christel and Katoen, Joost-Pieter and Larsen, Kim Guldstrand},
  year={2008},
  publisher={MIT press}
}

@inproceedings{chinchali2017multi,
author = {Chinchali, S. P. and Livingston, S. C. and Pavone, M.},
title = {Multi-objective optimal control for proactive decision-making with temporal logic models},
booktitle = {\textit{Int. Symp. on Robotics Research}},
year = {2017},
location = {Puerto Varas, Chile},
month = {dec},
url = {https://asl.stanford.edu/wp-content/papercite-data/pdf/Chinchali.Livingston.Pavone.ISRR17.pdf},
owner = {pavone},
timestamp = {2018-01-16},
}

@article{toussaint2010expectation,
  title={Expectation-Maximization methods for solving (PO) MDPs and optimal control problems},
  author={Toussaint, Marc and Storkey, Amos and Harmeling, Stefan},
  journal={Inference and Learning in Dynamic Models},
  year={2010},
  publisher={Cambridge University Press Cambridge}
}

@incollection{bandyopadhyay2013intention,
  title={Intention-aware motion planning},
  author={Bandyopadhyay, Tirthankar and Won, Kok Sung and Frazzoli, Emilio and Hsu, David and Lee, Wee Sun and Rus, Daniela},
  booktitle={Algorithmic Foundations of Robotics X},
  pages={475--491},
  year={2013},
  publisher={Springer}
}

@book{de1997formal,
  title={Formal verification of probabilistic systems},
  author={De Alfaro, Luca},
  number={1601},
  year={1997},
  publisher={stanford university}
}

@article{kupferman2001model,
  title={Model checking of safety properties},
  author={Kupferman, Orna and Vardi, Moshe Y},
  journal={Formal Methods in System Design},
  volume={19},
  number={3},
  pages={291--314},
  year={2001},
  publisher={Springer}
}

@article{boyd2003subgradient,
  title={Subgradient methods},
  author={Boyd, Stephen and Xiao, Lin and Mutapcic, Almir},
  year={2003},
  publisher={stanford university}
}

@article{Boehmer2016NonDeterministicPolicy,
	author={Wendelin BÃ¶hmer and Rong Guo and Klaus Obermayer},
	year={2016},
	title={Non-Deterministic Policy Improvement Stabilizes Approximated Reinforcement Learning},
	abstract={This paper investigates a type of instability that is linked to the greedy policy improvement in approximated reinforcement learning. We show empirically that non-deterministic policy improvement can stabilize methods like LSPI by controlling the improvements' stochasticity. Additionally we show that a suitable representation of the value function also stabilizes the solution to some degree. The presented approach is simple and should also be easily transferable to more sophisticated algorithms like deep reinforcement learning.},
	language={English},
}
@book{Sugiyama2015StatisticalRL,
	author={Masashi Sugiyama and Safari Books Online},
	year={2015},
	title={Statistical reinforcement learning: modern machine learning approaches},
	publisher={CRC Press, Taylor \& Francis Group},
	address={Boca Raton, FL},
	edition={1},
	isbn={1439856893},
	language={English}
}

@article{Hanawal2017LearningPolicies,
	author={Manjesh K. Hanawal and Hao Liu and Henghui Zhu and Ioannis Ch Paschalidis},
	year={2017},
	month={01/},
	title={Learning Policies for Markov Decision Processes from Data},
	language={English},
}

@article{chen2016statistical,
  title={Statistical inference for model parameters in stochastic gradient descent},
  author={Chen, Xi and Lee, Jason D and Tong, Xin T and Zhang, Yichen},
  journal={arXiv preprint arXiv:1610.08637},
  year={2016}
}

@article{li2017statistical,
  title={Statistical inference using SGD},
  author={Li, Tianyang and Liu, Liu and Kyrillidis, Anastasios and Caramanis, Constantine},
  journal={arXiv preprint arXiv:1705.07477},
  year={2017}
}

@inproceedings{nachum2017bridging,
  title={Bridging the gap between value and policy based reinforcement learning},
  author={Nachum, Ofir and Norouzi, Mohammad and Xu, Kelvin and Schuurmans, Dale},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2772--2782},
  year={2017}
}
@inproceedings{lim2013reinforcement,
  title={Reinforcement learning in robust markov decision processes},
  author={Lim, Shiau Hong and Xu, Huan and Mannor, Shie},
  booktitle={Advances in Neural Information Processing Systems},
  pages={701--709},
  year={2013}
}

@book{hernandez2012adaptive,
	title={Adaptive Markov control processes},
	author={Hern{\'a}ndez-Lerma, On{\'e}simo},
	volume={79},
	year={2012},
	publisher={Springer Science \& Business Media}
}

@article{bertuccelli2012robust,
	title={Robust adaptive Markov decision processes: Planning with model uncertainty},
	author={Bertuccelli, Luca F and Wu, Albert and How, Jonathan P},
	journal={IEEE Control Systems},
	volume={32},
	number={5},
	pages={96--109},
	year={2012},
	publisher={IEEE}
}

@article{brafman2002r,
	title={R-max-a general polynomial time algorithm for near-optimal reinforcement learning},
	author={Brafman, Ronen I and Tennenholtz, Moshe},
	journal={Journal of Machine Learning Research},
	volume={3},
	number={Oct},
	pages={213--231},
	year={2002}
}

@inproceedings{doshi2016hidden,
	title={Hidden parameter Markov decision processes: A semiparametric regression approach for discovering latent task parametrizations},
	author={Doshi-Velez, Finale and Konidaris, George},
	booktitle={IJCAI: proceedings of the conference},
	volume={2016},
	pages={1432},
	year={2016},
	organization={NIH Public Access}
}

@inproceedings{killian2017robust,
	title={Robust and Efficient Transfer Learning with Hidden Parameter Markov Decision Processes},
	author={Killian, Taylor W and Daulton, Samuel and Konidaris, George and Doshi-Velez, Finale},
	booktitle={Advances in Neural Information Processing Systems},
	pages={6250--6261},
	year={2017}
}

@article{kaelbling1998planning,
	title={Planning and acting in partially observable stochastic domains},
	author={Kaelbling, Leslie Pack and Littman, Michael L and Cassandra, Anthony R},
	journal={Artificial intelligence},
	volume={101},
	number={1-2},
	pages={99--134},
	year={1998},
	publisher={Elsevier}
}

@inproceedings{Fu-RSS-14, 
	AUTHOR    = {Jie Fu AND Ufuk Topcu}, 
	TITLE     = {Probably Approximately Correct MDP Learning and Control With Temporal Logic Constraints}, 
	BOOKTITLE = {Proceedings of Robotics: Science and Systems}, 
	YEAR      = {2014}, 
	ADDRESS   = {Berkeley, USA}, 
	MONTH     = {July},
	DOI       = {10.15607/RSS.2014.X.039} 
} 

@inproceedings{lim2013reinforcement,
	title={Reinforcement learning in robust markov decision processes},
	author={Lim, Shiau Hong and Xu, Huan and Mannor, Shie},
	booktitle={Advances in Neural Information Processing Systems},
	pages={701--709},
	year={2013}
}

@inproceedings{herman2016inverse,
	title={Inverse reinforcement learning with simultaneous estimation of rewards and dynamics},
	author={Herman, Michael and Gindele, Tobias and Wagner, J{\"o}rg and Schmitt, Felix and Burgard, Wolfram},
	booktitle={Artificial Intelligence and Statistics},
	pages={102--110},
	year={2016}
}

@incollection{williams1992simple,
	title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
	author={Williams, Ronald J},
	booktitle={Reinforcement Learning},
	pages={5--32},
	year={1992},
	publisher={Springer}
}

@article{tangkaratt2014model,
	title={Model-based policy gradients with parameter-based exploration by least-squares conditional density estimation},
	author={Tangkaratt, Voot and Mori, Syogo and Zhao, Tingting and Morimoto, Jun and Sugiyama, Masashi},
	journal={Neural networks},
	volume={57},
	pages={128--140},
	year={2014},
	publisher={Elsevier}
}

@article{kingma2014adam,
	title={Adam: A method for stochastic optimization},
	author={Kingma, Diederik P and Ba, Jimmy},
	journal={arXiv preprint arXiv:1412.6980},
	year={2014}
}

@article{sehnke2010parameter,
	title={Parameter-exploring policy gradients},
	author={Sehnke, Frank and Osendorfer, Christian and R{\"u}ckstie{\ss}, Thomas and Graves, Alex and Peters, Jan and Schmidhuber, J{\"u}rgen},
	journal={Neural Networks},
	volume={23},
	number={4},
	pages={551--559},
	year={2010},
	publisher={Elsevier}
}

@article{peters2008reinforcement,
	title={Reinforcement learning of motor skills with policy gradients},
	author={Peters, Jan and Schaal, Stefan},
	journal={Neural networks},
	volume={21},
	number={4},
	pages={682--697},
	year={2008},
	publisher={Elsevier}
}

@inproceedings{konda2000actor,
	title={Actor-critic algorithms},
	author={Konda, Vijay R and Tsitsiklis, John N},
	booktitle={Advances in neural information processing systems},
	pages={1008--1014},
	year={2000}
}

@inproceedings{deisenroth2011pilco,
	title={PILCO: A model-based and data-efficient approach to policy search},
	author={Deisenroth, Marc and Rasmussen, Carl E},
	booktitle={Proceedings of the 28th International Conference on machine learning (ICML-11)},
	pages={465--472},
	year={2011}
}

@inproceedings{silver2009monte,
	title={Monte-Carlo simulation balancing},
	author={Silver, David and Tesauro, Gerald},
	booktitle={Proceedings of the 26th Annual International Conference on Machine Learning},
	pages={945--952},
	year={2009},
	organization={ACM}
}

@inproceedings{ross2011reduction,
	title={A reduction of imitation learning and structured prediction to no-regret online learning},
	author={Ross, St{\'e}phane and Gordon, Geoffrey and Bagnell, Drew},
	booktitle={Proceedings of the fourteenth international conference on artificial intelligence and statistics},
	pages={627--635},
	year={2011}
}

@article{polydoros2017survey,
	title={Survey of model-based reinforcement learning: Applications on robotics},
	author={Polydoros, Athanasios S and Nalpantidis, Lazaros},
	journal={Journal of Intelligent \& Robotic Systems},
	volume={86},
	number={2},
	pages={153--173},
	year={2017},
	publisher={Springer}
}

@inproceedings{andersen2017active,
	title={Active Exploration for Learning Symbolic Representations},
	author={Andersen, Garrett and Konidaris, George},
	booktitle={Advances in Neural Information Processing Systems},
	pages={5016--5026},
	year={2017}
}

@inproceedings{khamassi2017active,
	title={Active exploration and parameterized reinforcement learning applied to a simulated human-robot interaction task},
	author={Khamassi, Mehdi and Velentzas, George and Tsitsimis, Theodore and Tzafestas, Costas},
	booktitle={Robotic Computing (IRC), IEEE International Conference on},
	pages={28--35},
	year={2017},
	organization={IEEE}
}

@inproceedings{andersson2017deep,
	title={Deep Learning Quadcopter Control via Risk-Aware Active Learning.},
	author={Andersson, Olov and Wzorek, Mariusz and Doherty, Patrick},
	booktitle={AAAI},
	pages={3812--3818},
	year={2017}
}

@inproceedings{martinez2007active,
	title={Active Policy Learning for Robot Planning and Exploration under Uncertainty.},
	author={Martinez-Cantin, Ruben and de Freitas, Nando and Doucet, Arnaud and Castellanos, Jos{\'e} A},
	booktitle={Robotics: Science and Systems},
	volume={3},
	pages={334--341},
	year={2007}
}
