%
% Since this is a ``report'', the topmost level of hierarchy is
% ``Chapter'', not section as you may be used to. Chapters are
% enumerated starting from 1, so Sections are 1.1, Subsections are
% 1.1.1, subsubsections don't get numbers. (You can change that, if
% you want them to be called 1.1.1.1)
%
\chapter{Motivation for Active Policy Inference}\label{chapt:motivation}

%\todo[inline, color=green]{
%                           Throughout this document, I precede each section with a note about the section. That note
%                           should explain the goal of the section, e.g., what I'm intending to convey in the section.
%                           When reviewing, please consider if the section adequately addresses the following points:
%
%                           1) Does the section meet the goal I stated.
%
%                           2) Is this the correct place in the document for that goal to be fulfilled (\eg, is it too
%                              early or too late in the document.
%
%                           3) If you're reading this, I'm already grateful for your help.
%                          }
%
%\todo[inline]{
%              My goal in this section is to provide motivation for the problem I have addressed in this thesis. I
%              introduce a high level example, and then discuss current literature.
%             }

    A fundamental assumption of this thesis is that when an autonomous agent moves in an environment it is attempting to
    fulfill a specified task. The agent could be an autonomous vehicle or a robotic manipulator and in a realistic
    environment, there will be static obstacles and other autonomous and uncontrollable agents. Tasks for autonomous
    agents are often multifaceted. Two examples are:
    \begin{itemize}
        \item \quotationMarks{Drive from point-A to point-B
    and neither leave the road nor collide with another car or pedestrian.}
        \item \quotationMarks{Pick up everything on the table and put it in the box that the human is holding.}
   \end{itemize}

        It is clear that
    accurately representing the probabilistic motion distribution of uncontrollable agents -- cars, pedestrians, or
    people holding boxes -- will benefit the autonomous agent as it tries to complete its main objective. This thesis
    details an algorithm that elicits informative interactions with an uncontrollable agent that a controllable agent
    uses to effectively plan its future actions.

\section{Introduction}\label{sec:introduction}

    Modeling stochastic environments with discrete state and action spaces has traditionally been accomplished with
    \iac{MDP}. An agent that takes planned actions in \iac{MDP} is following its \textit{policy}. When an autonomous
    agent is given a task to complete in its environment, the agent is rewarded upon completing this task. In a
    realistic scenario, there are often too many unknowns to solve for a control policy that has the highest likelihood
    of returning a reward. The agent may not know how the actions it can take affect its transitions from one state to
    another and, therefore, can't immediately make an efficient plan. Or, the agent may know what actions to take to
    earn the reward, but environmental disturbances require that the agent adapt. In the latter case, the agent needs to
    learn a model of its environment.

    In this work, we'll assume that a stochastic environment has been successfully modeled, and that, given an arbitrary
    task, our controllable agent can solve for an optimal policy that maximizes the likelihood of reward. However, when
    we introduce a second, uncontrollable, agent into the environment the controllable agent needs learn how the
    uncontrollable agent will affect it's plan. We build upon \textit{policy gradient} methods to infer the policy of
    the uncontrolable agent, and subsequently converge to the optimal policy for the controllable agent over the course
    of many interactions. This implementation parameterizes the policy with a multivariate-normal distribution. We use
    the learned policy-parameter variance  to quantify the uncertainty in the inferred parameters, and use that to guide
    exploration.

    In Sections \ref{sec:literature_hipmdp}-\ref{sec:contributions} we'll cover the how current models and algorithms
    built the foundation for this policy inference algorithm. Section \ref{sec:hipmdp} will cover common notation in
    \ac{RL} and \ac{MDP} to serve as preliminary information to the main result in Chapter \ref{chapt:gauss_policy}.
    Here, we'll test the inference algorithm by inferring the policy of a single agent. The following chapter,
    \ref{chapt:multi_agent}, will extend the model and experiments to two agents. Finally, Chapter
    \ref{chapt:proactive_inference} will introduce our algorithm for active policy inference and demonstrate results
    with several simulation experiments.

    Consider the following: Each day, a mobile robot repeatedly traverses a factory floor to deliver a parcel.
    Eventually, a second robot also starts working in the area, and the first robot does not know anything about the
    second robot's task. The first robot must not collide with the second robot and, without knowledge of the second
    robot's intent, it now takes longer to deliver its parcel each day. After each day, the first robot can use it's
    observations of the inter-robot interactions to improve its delivery plan. Is there a better solution than greedily
    updating the plan?  Can the first robot plan to move in such a way that will expose more about the second robot?

\section{Inferring Latent Variables in MDPs}\label{sec:literature_hipmdp}


    Given an unknown system that is modeled as \iac{MDP}, adaptive planning \cite{hernandez2012adaptive} means that the
    agent must learn or infer latent (hidden) environmental parameters and replan when it has a new estimate of the
    unknown system. The main techniques to accomplish this fall into two distinct categories, model-\textit{based}
    \ac{RL}, and model-\textit{free} \ac{RL}; the survey in \cite{polydoros2017survey} provides a succinct comparison.
    Generally, model-based \ac{RL} attemts to learn the underlying transition model. By learning an approximatly correct
    environemntal model, an agent can optimally solve problems
    \cite{Fu-RSS-14}\cite{bertuccelli2012robust}\cite{deisenroth2011pilco}. Model-free \ac{RL} does not learn an
    environmental model, but learns the actions, or control inputs, to take at given states by performing gradient
    ascent on the likelihood of a reward function \cite{williams1992simple}\cite{peters2008reinforcement}. In general,
    the \ac{RL} comunity is now focusing on methods that combine model-free with model-based approaches called
    actor-critic methods \cite{konda2000actor}. Actor-critic methods update an estimated environmental model to help
    reduce the variance in the policy-gradient common in model-free implementations \cite{peters2008reinforcement}.

\subsection{Model Based Solutions}\label{sec:model_based_lit}

    For the described multi-agent factory scenario, suppose that there are a set of parameters that can characterize the
    policy of the second, unknown and uncontrollable, robot. To capture these unknown parameters, we'll extend the model
    from \iac{MDP} to a \ac{HiPMDP}    \cite{doshi2016hidden}. \ac{HiPMDP}s are well suited to problems where the number
    of parameters to learn is small relative to the size of the state-action space. This is a very applicable framework
    when certain aspects of the environment are already known, and sensory information is precise and accurate.

    Arguably, simply exploiting information from a multi-agent interaction only requires a controllable agent to capture
    the \textit{intent} of an uncontrollable agent.  Recently, the \ac{MOMDP}has been presented in
    \cite{bandyopadhyay2013intention} as a an instantiation of a \ac{POMDP}\cite{kaelbling1998planning}. In this
    \ac{MOMDP}, an autonomous golf-cart maintains a belief over a pedestrian's intent when they interact. The authors
    state that \quotationMarks{the [golf-cart]'s ultimate goal is to complete the specified task and not to recognize
    intention.} In other words, learning the policy of an uncontrollable agent provides excessive information to handle
    a vehicle-pedestrian encounter in a crosswalk.


    In comparison to \ac{HiPMDP} and \ac{MOMDP}, more general classes of \ac{RL} algorithms assume that the entire
    transition model is unknown. To prove that a transition model has been learned,  \cite{Fu-RSS-14} puts guarantees on
    the samples required to achieve a \ac{pacmdp}. The authors show that the number of samples needed to learn a
    transition system in an \ac{MDP} that has a bounded model error scales polynomially.  The authors also present an
    \ac{MLE} of the transition model parameters, complete with empirical mean and variance, but this requires a
    parameter set that scales with the size of the state and action domains.

    The \Rmax algorithm from \cite{brafman2002r} is specifically geared to solve a multi-agent \ac{MDP}. \Rmax, however,
    would require that the controllable agent coerce visits to each \textit{joint}-state\footnotemark enough times to
    conclude, with a specified confidence, what the probability of future joint states is. This essentially models the
    action distribution of the other agent(s) as part of the underlying transition model.  \footnotetext{A joint-state
    is the combination of the states of individual agents.}

    Using another approach in the face of uncertain transitions, \cite{bertuccelli2012robust} merge adaptive and robust
    methods for solving \acp{MDP}. Normally a robust solution uses the \textit{minimax} approach, it plans for the
    worst-case transition. Instead of planning for the worst, \iac{uav} models its uncertain transitions with Dirichlet
    distributions. In an algorithm like an \ac{ekf}, transition samples form an empirical covariance matrix of the
    system's transition model so that the \ac{uav} can reduce its exposure to a failed mission. This allows for the
    system to have a failure \emph{risk-tolerance} instead of using a conservative \emph{minimax} plan.

    The objective of \cite{chinchali2017multi} is learn the true model of several adversaries by minimizing
    \quotationMarks{the cost of information gain}. Each adversarial agent is modeled with an \ac{MDP} and a set of
    pre-designed temporal logic specifications. The goal is to identify which specification each adversary is trying to
    satisfy. The controllable agent receives a reward by maximizing the information gain, the entropy of the current
    state belief.

    Similarly, \cite{lim2013reinforcement} assume that an adversary in \iac{MDP} may have a non-stationary policy. If
    the adversary chooses to play \quotationMarks{nicely}, then a minimax policy would be far too conservative. Each
    time a state transition is observed, the authors record it and subject each new transition to a
    \quotationMarks{stochasticity check}, essentially trying to detect a change in the adversarial policy using the
    Chernoff bound. Both \cite{lim2013reinforcement} and \cite{bertuccelli2012robust} have tried to overcome the
    conservativeness often encountered in \textit{minimax} policies, but environmental agents can be agnostic, or even
    complimentary, to the controllable agent's intent.

    In general, model-based solutions allow the agent to leverage its belief about future states as it plans. This
    belief is reflected in the \textit{value} of each state, but is very sensitive to model errors.

    In this work, we assume that the second agent's policy is unknown but can be parameterized. We'll use a \ac{HiPMDP}
    to learn the policy from observed state-sequences; we do not know what actions the agent has decided to take at each
    state. In single agent scenarios, \cite{Fu-RSS-14}, \cite{bertuccelli2012robust}, \cite{peters2008reinforcement},
    and \cite{tangkaratt2014model}, the agent always knows what action it just took. This work shows that we can learn
    the hidden action-distribution of an uncontrollable agent, while only observing the outcomes of those actions.

\subsection{Exploration in Policy Gradient Methods}

    If one agent needs to interact with another agent to learn that agent's policy, then some level of ``exploration''
    will help it learn faster \cite{nachum2017bridging}. Instead of always taking an action on the path towards the
    highest reward, an agent can sometimes take \emph{sub}-optimal actions that might provide more information about
    hidden parameters.

    This work will implement policy-gradient methods to infer a policy from observed state-sequences. Policy-gradients
    \cite{williams1992simple} are traditionally applied in model-\emph{free} \ac{RL} as well as actor-critic \ac{RL}.
    Although this work will use policy-gradients in model-based learning, it is worthwhile to describe how exploration
    is achieved by other policy-gradient implementations, and how it produces more informative samples
    (state-sequences).

    The PILCO algorithm, from \cite{deisenroth2011pilco}, claims exceptional sample efficiency as it learns a dynamics
    model of a physical cart-pole swing-up. It learns the optimal control inputs to the system via a policy-gradient,
    but incorporates the variance of policy parameters into the policy synthesis by modeling the unknown dynamics as
    \acp{GP}.

    Many of these algorithms have relied on the random-sampling of policy parameters to effectively add entropy to their
    learning process. For instance, \cite{peters2008reinforcement} use a stochastic disturbance acceleration policy to
    control robot manipulator servos. Similarly, \cite{sehnke2010parameter} samples actions from a distribution, with a
    mean value determined by learned feature weightings.

    If an agent is synthesizing a policy, regularizing a value function with an entropy term puts value on exploration
    \cite{nachum2017bridging}. Entropy regularization works in-practice but this is a heuristic that does
    not \textit{guide} exploration in a formulaic approach. This work will infer the policy of an uncontrollable agent
    using a policy-gradient method, and directly use the second moment of the policy parameters to guide exploration.


\subsection{Learning from Demonstration}

    The stated inference problem is similar to \emph{imitation learning}, also referred to as \textit{learning from
    demonstration}. These algorithms attempt to learn a policy from expert demonstrations, although the
    \quotationMarks{expert} may be following a sub-optimal policy.

    Given a set of state-action samples and a set of pre-defined features, \cite{Hanawal2017LearningPolicies} discuss
    the number of samples required to learn an unknown transition model \emph{and} policy from demonstration given a
    maximum model-error threshold. The authors can also estimate the log-loss function used by the expert.

    In an imitation learning approach, the \DAGGER algorithm learns to complete a task in a supervised fashion
    \cite{ross2011reduction}. Given an initial set of trajectories from an expert, the agent then attempts to mimic the
    policy. When an agent is trying to mimic an expert's policy, the agent's probability of making a ``wrong'' dicision
    over the course of a trajectory grows polynomially with the length of the trajectory. \DAGGER will minimize that
    probability of error by minimizing an upper bound to the 0-1 loss of a learned trajectory; the 0-1 loss would count
    the number of times the agent took a different action than the expert, but this is not always observable.

    At any point during a trajectory, the agent has some probability of asking for the expert for a correction that will
    last a few time steps, creating a mixture of expert and agent decisions.  Early in the learning, the agent is likely
    to have made several errors, so the expert correction is very informative. The correction will probably start from a
    state that the expert would not have visited on its own, and the chance of querying the expert for advice diminishes
    as the learning procedure continues.

    By aggregating the observed data from previous learning stages, \DAGGER produces a final policy that matches the
    average loss incurred during the training period. Since the first stages had a high proportion of expert
    trajectory-segments, this final policy is comparable to the expert's. As the number of stages goes to infinity, the
    loss of the final policy will converge to the loss of the \emph{best} policy.

\subsection{Active Learning}

    When an agent is trying to learn a model, e.g. a transition or value function, collecting data from areas of the
    largest model error is clearly the best way to improve a model estimate. The true model error is rarely available to
    an agent, however. As a proxy, agents can minimize an upper bound to that error, or seek data that minimizes the
    uncertainty in the model itself. An agent that intentially seeks data that will minimize that error or uncertainty
    is employing an \emph{active} procedure in the \ac{RL} task. In fact, the \DAGGER algorithm \cite{ross2011reduction}
    previously discussed has been described as \quotationMarks{active leraning from an oracle} by
    \cite{andersson2017deep}. Several other active learning implementations are discussed here.

    Active learning is used to provide risk-aware trajectory sampling for a quadcopter in \cite{andersson2017deep}. The
    quadcopter must learn a policy that minimizes a loss function that penalizes collisions with uncontrollable humans
    and the distance to a target \cite{andersson2017deep}. The authors assert that there is a subset of states where the
    learned policy must be as close to optimal as possible, while other subsets can accept larger error. The most
    crucial states are those that have a high risk of catastrophic loss; in \cite{andersson2017deep}'s experiment, these
    are states where the quadcopter is close to colliding with humans. The quadcopter gathers trajectory samples in the
    environment and then trains a policy with a deep neural network based on a loss-function. New trajectories are
    sampled to start in areas of high loss. While this implementation quickly learned how to avoid collisions and reach
    a target, much of this work unsupported by any rigorous proofs and the degree of active sampling is controlled by
    the system designer.

    In comparison, \cite{khamassi2017active} randomly sample policy parameters from a normal distribution and actively
    update the distributions movements as a function of the rewards earned by recent trajectory samples from a system. A
    robot interacts with a human and is rewarded by an \emph{engagement} metric. This implementation actively varies the
    distribution of policies that generate the trajectories to converge to the most rewarding policy, but the mechanism
    to control this active update is still heuristically defined.

    A more rigorous application of active learning is applied to a \ac{slam} problem in \cite{martinez2007active}. A
    robot is tasked to reach a target location and while the loss-function includes is the uncertainty in the robot's
    final position, it also includes the uncertainty in the locations of several objects in an unknown world. The robot
    learns policy parameters (waypoints) that lead to a target position and the loss is represented as the trace of the
    covariance matrix on the robot and object location estimates. Additionally, each object's location can only be
    sensed (corrupted by noise) from a subset of the robot state space. After reaching each waypoint, the robot actively
    uses a receding horizon planner to select future waypoints. The policy parameters are modeled by \acf{GP}. The
    distribution that future waypoints are chosen from is proportional to the infill of the \ac{GP} that defines the
    remaining policy parameters to be chosen. Infill is represented as the amount of the \ac{GP}'s
    cumulative-density-function that lies below the minimum loss observed in the current trajectory. Therfore, the next
    waypoints are chosen proportional to the amount of valuable information that can be gained about object and robot
    locations.

    Finally, \cite{akiyama2010efficient} apply the \ac{lspi} and active-learning to a robot learning to hit a ball with
    a bat. The derivation is motivated by sample efficiency, especially for cases when gathering trajectories is cheap,
    but computing rewards for trajectories is expensive. The authors provide a consistent estimator of reward error, the
    true-reward less the expected-reward generated by a trajectory. They assume that, given their feature selection,
    there are optimal parameters that can represent the optimal \emph{Q}-function. First, policies are sampled, and for
    each policy, a set of trajectories are generated without evaluating a reward. The consistent estimator of reward
    error is the trace of the empirical (importance weighted) covariance of the expected discounted features over all
    sampled trajectories, and the policy that minimizes this loss is used to gather rewarded-trajectories; this policy
    is used to try to hit the ball. This \emph{active} synthesis step is repeated after new (unrewarded) trajectories
    are gathered, and used as the basis for importance weighting, but it is not used for sampling the trajectories so
    the collection of trajectories remains statistically independent. So, this implementation actively \emph{weights}
    sampled trajectories to eventually learn an optimal policy.

    The key tenant of (most) active learning algorithms is determining what is unknown in a model and adjusting the
    policy such that some measure of the unknown is minimized. This helps active learning algorithms improve sample
    efficiency in \ac{RL}.

    \begin{remark}
        It should be noted that \cite{friston2012active} uses the term ``active inference'' to describe a policy
        synthesis algorithm for computing an optimal policy in a belief-\ac{MDP}. This synthesis algorithm shows that
        minimizing \textnormal{surprise}, the negated log-likelihood of an observed state transition, yields the same
        policy as maximizing the expected reward. This should not be confused with the implementation in this work.
%    \begin{itemize}
%        \item claim \emph{active inference} a policy should minimize the variational free energy on the marginal
%            likelihood of observed states
%        \item first, they show that an optimal control problem can be formulated as a Bayesian inference problem
%        \item ``action does not minimize a cost, but rather maximizes the marginal likelihood of observations under a
%            generative model that entails a policy''
%        \item ``active inference elicits observations that are most plausible under beliefs about future states'', this
%        essentially minimizes surprise (compared to eliciting future states based on value) \item formulated as a POMDP
%            where all states are hidden and the agent makes stochastic observations of the current state
%        \item to minimize ``surprise'' this method minimizes the expected negated log-likelihood of an observation,
%            marginalized over hidden states in a model.
%        \item cost functions are replaced by priors over hidden states and transitions such that they are very
%            surprising, such that this formulation will avoid taking actions that lead to a belief that the system is in
%            this surprising state
%        \item the term ``active infernce'' in this work and others by Friston et. al, is used to note that an optimal
%            policy can be synthesized in a partially observable environment by minimizing the surprise of state
%            transitions. However, the note that this relies on the agent being provided with a probabilistic model of
%            the environment. In this thesis we will provide a framework for actively inferring the \emph{probabalistic
%            transition model}.
%    \end{itemize}
    \end{remark}

\section{Contributions of this work}\label{sec:contributions}

    We apply model-based \ac{RL} to a problem where one agent must learn the hidden policy of an uncontrollable agent
    through interaction. The policy of the uncontrolable agent is fixed, it does not change with time, but might be
    dependent on the relative position of the the controllable agent. The environment is stochastic, actions lead to a
    distribution over future states. By learning the policy of the uncontrollable agent, the controllable agent can
    optimally plan to complete its task.

    We contribute three major advances. First, we apply a policy-gradient method to infer the demonstrated policy of the
    uncontrollable agent. The inferred policy is parameterized as a multivariate distribution that allows this
    implementation to capture the second-moment of the policy-distribution. The second-moments quantify the ambiguity in
    the inferred policy. Second,  we'll request the uncontrollable agent to sample its initial state based on the
    ambiguity in the policy-distribution. This provides guide for efficiently requesting policy demonstrations. Third,
    we'll show that a controllable agent can adaptively plan to proactively infer the policy  of the uncontrollable
    agent. The controllable agent will receive a bonus reward when states with a high uncertainty are visited.

    Unlike \cite{lim2013reinforcement} and \cite{bertuccelli2012robust}, we do not limit the case to adversaries; there
    is a chance that the uncontrollable agent will help earn rewards. Finally, we perform the inference with only
    state-sequences. The actions taken by the uncontrolable agent are not observable, only the action-outcome.

