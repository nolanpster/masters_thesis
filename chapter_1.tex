%
% Since this is a ``report'', the topmost level of hierarchy is
% ``Chapter'', not section as you may be used to. Chapters are
% enumerated starting from 1, so Sections are 1.1, Subsections are
% 1.1.1, subsubsections don't get numbers. (You can change that, if
% you want them to be called 1.1.1.1)
%
\chapter{Motivation for Active Policy Inference}\label{chapt:motivation}

%\todo[inline, color=green]{
%                           Throughout this document, I precede each section with a note about the section. That note
%                           should explain the goal of the section, e.g., what I'm intending to convey in the section.
%                           When reviewing, please consider if the section adequately addresses the following points:
%
%                           1) Does the section meet the goal I stated.
%
%                           2) Is this the correct place in the document for that goal to be fulfilled (\eg, is it too
%                              early or too late in the document.
%
%                           3) If you're reading this, I'm already grateful for your help.
%                          }
%
%\todo[inline]{
%              My goal in this section is to provide motivation for the problem I have addressed in this thesis. I
%              introduce a high level example, and then discuss current literature.
%             }

    A fundamental assumption of this thesis is that when an autonomous agent moves in an environment it is attempting to
    fulfill a specified task. The agent could be an autonomous vehicle or a robotic manipulator and in a realistic
    environment there will be static obstacles and other autonomous and uncontrollable agents. Tasks for autonomous
    agents are often multifaceted. Two examples are:
    \begin{itemize}
        \item \quotationMarks{Drive from point-A to point-B
    and do not leave the road or collide with another car or pedestrian.}
        \item \quotationMarks{Pick up everything on the table and put it in the box that the human is holding.}
   \end{itemize}

        It is clear that
    accurately representing the probabilistic motion distribution of uncontrollable agents -- cars, pedestrians, or
    people holding boxes -- will benefit the autonomous agent as it tries to complete its main objective. This thesis
    details an algorithm that elicits informative interactions with an uncontrollable agent that a controllable agent
    uses to effectively plan its future actions.

\section{Introduction}\label{sec:introduction}

    Modeling stochastic environments with discrete state and action spaces has traditionally been accomplished with
    \iac{MDP}. An agent that takes planned actions in \iac{MDP} is following its \textit{policy}. When an autonomous
    agent is given a task to complete in its environment, the agent is rewarded upon completing this task. In a
    realistic scenario, there are often too many unknowns to solve for a control policy that has the highest likelihood
    of returning a reward. The agent may not know how the actions it can take affect its transitions from one state to
    another and, therefore, can't immediately make an efficient plan. Or, the agent may know what actions to take to
    earn the reward, but environmental disturbances require that the agent adapt. In the latter case, the agent needs to
    learn a model of its environment.

    In this work, we'll assume that a stochastic environment has been successfully modeled, and that, given an arbitrary
    task, our controllable agent can solve for an optimal policy that maximizes the likelihood of reward. However, when
    we introduce a second, uncontrollable, agent into the environment the controllable agent needs learn how the
    uncontrollable agent will affect it's plan. We build upon \textit{policy gradient} methods to infer the policy of
    the uncontrolable agent, and subsequently converge to the optimal policy for the controllable agent over the course
    of many interactions. This implementation parameterizes the policy with a multivariate-normal distribution. We use
    the learned policy-parameter variance  to quantify the uncertainty in the inferred parameters, and use that to guide
    exploration.

    In Sections \ref{sec:literature_hipmdp}-\ref{sec:contributions} we'll cover the how current models and algorithms
    built the foundation for this policy inference algorithm. Section \ref{sec:hipmdp} will cover common notation in
    \ac{RL} and \ac{MDP} to serve as preliminary information to the main result in Chapter \ref{chapt:gauss_policy}.
    Here, we'll test the inference algorithm by inferring the policy of a single agent. The following chapter,
    \ref{chapt:multi_agent}, will extend the model and experiments to two agents. Finally, Chapter
    \ref{chapt:proactive_inference} will introduce our algorithm for active policy inference and demonstrate results
    with several simulation experiments.

    Consider the following. Each day, a mobile robot repeatedly traverses a factory floor to deliver a parcel.
    Eventually, a second robot also starts working in the area, and the first robot does not know anything about the
    second robot's task. The first robot must not collide with the second robot and, without knowledge of the second
    robot's intent, it now takes longer to deliver it's parcel each day. After each day, the first robot can use it's
    observations of the inter-robot interactions to improve its delivery plan. Is there a better solution than greedily
    updating the plan?  Can the first robot plan to move in such a way that will expose more about the second robot?

\section{Inferring Latent Variables in MDPs}\label{sec:literature_hipmdp}


    Given an unknown system that is modeled as \iac{MDP}, adaptive planning \cite{hernandez2012adaptive} means that the
    agent must learn or infer latent (hidden) environmental parameters and replan when it has a new estimate of the
    unknown system. The main techniques to accomplish this fall into two distinct categories, model-\textit{based}
    \ac{RL}, and model-\textit{free} \ac{RL}; the survey in \cite{polydoros2017survey} provides a succinct comparison.
    Generally, model-based \ac{RL} attemts to learn the underlying transition model. By learning an approximatly correct
    environemntal model, an agent can optimally solve problems
    \cite{Fu-RSS-14}\cite{bertuccelli2012robust}\cite{deisenroth2011pilco}. Model-free \ac{RL} does not learn an
    environmental model, but learns the actions, or control inputs, to take at given states by performing gradient
    ascent on the likelihood of a reward function \cite{williams1992simple}\cite{peters2008reinforcement}. In general,
    the \ac{RL} comunity is now focusing on methods that combine model-free with model-based approaches called
    actor-critic methods \cite{konda2000actor}. Actor-critic methods update an estimated environmental model to help
    reduce the variance in the policy-gradient common in model-free implementations \cite{peters2008reinforcement}.

\subsection{Model Based Solutions}\label{sec:model_based_lit}

    For the described multi-agent factory scenario, suppose that there are a set of parameters that can characterize the
    policy of the second, unknown and uncontrollable, robot. To capture these unknown parameters, we'll extend the model
    from \iac{MDP} to a \ac{HiPMDP}    \cite{doshi2016hidden}. \ac{HiPMDP}s are well suited to problems where the number
    of parameters to learn is small relative to the size of the state-action space. This is a very applicable framework
    when certain aspects of the environment are already known, and sensory information is precise and accurate.

    Arguably, simply exploiting information from a multi-agent interaction only requires a controllable agent to capture
    the \textit{intent} of an uncontrollable agent.  Recently, the \ac{MOMDP}has been presented in
    \cite{bandyopadhyay2013intention} as a an instantiation of a \ac{POMDP}\cite{kaelbling1998planning}. In this
    \ac{MOMDP}, an autonomous golf-cart maintains a belief over a pedestrian's intent when they interact. The authors
    state that \quotationMarks{the [golf-cart]'s ultimate goal is to complete the specified task and not to recognize
    intention.} In other words, learning the policy of an uncontrollable agent provides excessive information to handle
    a vehicle-pedestrian encounter in a crosswalk.


    In comparison to \ac{HiPMDP} and \ac{MOMDP}, more general classes of \ac{RL} algorithms assume that the entire
    transition model is unknown. To prove that a transition model has been learned,  \cite{Fu-RSS-14} puts guarantees on
    the samples required to achieve a \ac{pacmdp}. The authors show that the number of samples needed to learn a
    transition system in an \ac{MDP} that has a bounded model error scales polynomially.  The authors also present an
    \ac{MLE} of the transition model parameters, complete with empirical mean and variance, but this requires a
    parameter set that scales with the size of the state and action domains.

    The \Rmax algorithm from \cite{brafman2002r} is specifically geared to solve a multi-agent \ac{MDP}. \Rmax, however,
    would require that the controllable agent coerce visits to each \textit{joint}-states\footnotemark enough times to
    conclude, with a specified confidence, what the policy of the uncontrollable agent is. This essentially models the
    action distribution of the other agent(s) as part of the underlying transition model.  \footnotetext{A joint-state
    is the combination of the states of individual agents.}

    Using another approach in the face of uncertain transitions, \cite{bertuccelli2012robust} merge adaptive and robust
    methods for solving \acp{MDP}. Normally a robust solution uses the \textit{minimax} approach, it plans for the
    worst-case transition. Instead of planning for the worst, \iac{uav} models its uncertain transitions with Dirichlet
    distributions. In an algorithm like an \ac{ekf}, transition samples form an empirical covariance matrix of the
    system's transition model so that the \ac{uav} can reduce its exposure to a failed mission. This allows for the
    system to have a failure \emph{risk-tolerance} instead of using a conservative \emph{minimax} plan.

    The objective of \cite{chinchali2017multi} is learn the true model of several adversaries by minimizing
    \quotationMarks{the cost of information gain}. Each adversarial agent is modeled with an \ac{MDP} and a set of
    pre-designed temporal logic specifications. The goal is to identify which specification each adversary is trying to
    satisfy. The controllable agent receives a reward by maximizing the information gain, the entropy of the current
    state belief.

    Similarly, \cite{lim2013reinforcement} assume that an adversary in \iac{MDP} may have a non-stationary policy. If
    the adversary chooses to play \quotationMarks{nicely}, then a minimax policy would be far too conservative. Each
    time a state transition is observed, the authors record it and subject each new transition to a
    \quotationMarks{stochasticity check}, essentially trying to detect a change in the adversarial policy using the
    Chernoff bound. Both \cite{lim2013reinforcement} and \cite{bertuccelli2012robust} have tried to overcome the
    conservativeness often encountered in \textit{minimax} policies, but environmental agents can agnostic, or even
    complimentary, to the controllable agent's intent.

    In general, model-based solutions allow the agent to leverage it's belief about future states as it plans. This
    belief is reflected in the \textit{value} of each state, but is very sensitive to model errors.

    In this work, we assume that the second agent's policy is unknown but can be parameterized. We'll use a \ac{HiPMDP}
    to learn the policy from observed state-sequences; we do not know what actions the agent has decided to take at each
    state. In single agent scenarios, \cite{Fu-RSS-14}, \cite{bertuccelli2012robust}, \cite{peters2008reinforcement},
    and \cite{tangkaratt2014model}, the agent always knows what action it just took. This work shows that we can learn
    the hidden action-distribution of an uncontrollable agent, while only observing the outcomes of those actions.

\subsection{Exploration in Policy Gradient Methods}

    If one agent needs to interact with another agent to learn that agent's policy, then some level of ``exploration''
    will help it learn faster \cite{nachum2017bridging}. Instead of always taking an action on the path towards the
    highest reward, an agent can sometimes take \emph{sub}-optimal actions that might provide more information about
    hidden parameters.

    This work will implement policy-gradient methods to infer a policy from observed state-sequences. Policy-gradients
    \cite{williams1992simple} are traditionally applied in model-\emph{free} \ac{RL} as well as actor-critic \ac{RL}.
    Although this work will use policy-gradients in model-based learning, it is worthwhile to describe how exploration
    is used by other policy-gradient implementations, and how it produces more informative samples (state-sequences).


    The PILCO algorithm, from \cite{deisenroth2011pilco}, claims exceptional sample efficiency as it learns a dynamics
    model of a physical cart-pole swing-up. It learns the optimal control inputs to the system via a policy-gradient,
    but incorporates the variance of policy parameters into the policy synthesis by modeling the unknown dynamics as
    \acp{GP}.

    Many of these algorithms have relied on the random-sampling of policy parameters to effectively add entropy to their
    learning process. For instance, \cite{peters2008reinforcement} use a stochastic disturbance acceleration policy to
    control robot manipulator servos. Similarly, \cite{sehnke2010parameter} samples actions from a distribution, with a
    mean value determined by learned feature weightings.

    If an agent is synthesizing a policy, regularizing a value function with an entropy term puts value on exploration
    \cite{nachum2017bridging}. Entropy regularization works in-practice, but like applying random-restarts to a
    Q-learning algorithm \color{blue}(need a citation for this!)\color{black}, these are heuristic approaches that do
    not \textit{guide} exploration in a formulaic approach. This work will infer the policy of an uncontrollable agent
    using a policy-gradient method, and directly use the second moment of the policy parameters to guide exploration.


\subsection{Learning from Demonstration}

    The stated inference problem is similar to \emph{imitation learning}, also referred to as \textit{learning from
    demonstration}. These algorithms attempt to learn a policy from expert demonstrations, although the
    \quotationMarks{experts} may be following sub-optimal policies.

    Given a set of state-action samples and a set of pre-defined features, \cite{Hanawal2017LearningPolicies} discuss
    the number of samples required to learn an unknown transition model \emph{and} policy from demonstration given a
    maximum model-error threshold. The authors can also estimate the log-loss function used by the expert.

    In an imitation learning approach, the \DAGGER algorithm learns to complete a task in a supervised fashion
    \cite{ross2011reduction}. Given an initial set of trajectories from an expert, the agent then attempts to mimic the
    policy. When an agent is trying to mimic an expert's policy, the agent's probability of making a ``wrong'' dicision
    over the course of a trajectory grows polynomially with the length of the trajectory. \DAGGER will minimize that
    probability of error by minimizing an upper bound to the 0-1 loss of a learned trajectory; the 0-1 loss would count
    the number of times the agent took a different action than the expert, but this is not always observable.

    At any point during a trajectory, the agent has some probability of asking for the expert to correct the trajectory.
    Early in the learning, the agent is likely to have made several errors, so the expert correction is very
    informative. The correction will probably start from a state that the expert would not have visited on its own. The
    chance of querying the expert for advice diminishes as the learning procedure continues.

    By aggregating the observed data from previous learning stages, \DAGGER produces a final policy that matches the
    average loss incurred during the training period. Since the first stages had a high proportion of expert
    trajectory-segments, this final policy is comparable to the expert's. As the number of stages goes to infinity, the
    loss of the final policy will converge to the loss of the \emph{best} policy.

\subsection{Active Learning}

    \todo[inline]{To Do: provide overview of active learning}

    Using a semi-MDP,  \cite{andersen2017active} use a policy that is exploration only but non-Markovian.  By clustering
    states to symbols, where symbols can be chained together to represent high level task options, the exploration
    policy chooses actions that maximize the expected information gain.

    Planning to summarize the following:
    \begin{itemize}
        \item \cite{khamassi2017active}
        \item \cite{andersson2017deep}
        \item \cite{martinez2007active}
        \item \cite{akiyama2010efficient}
        \item re-read \cite{andersen2017active}
    \end{itemize}


\section{Contributions of this work}\label{sec:contributions}

    We apply model-based \ac{RL} to a problem where one agent must learn the hidden policy of an uncontrollable agent
    through interaction. The policy of the uncontrolable agent is fixed, it does not change with time, but might be
    dependent on the relative position of the the controllable agent. The environment is stochastic, actions lead to a
    distribution over future states. By learning the policy of the uncontrollable agent, the controllable agent can
    optimally plan to complete its task.

    We contribute three major advances. First, we apply a policy-gradient method to infer the demonstrated policy of the
    uncontrollable agent. The inferred policy is parameterized as a multivariate distribution that allows this
    implementation to capture the second-moment of the policy-distribution. The second-moments quantify the ambiguity in
    the inferred policy. Second,  we'll request the uncontrollable agent to sample its initial state based on the
    ambiguity in the policy-distribution. This provides guide for efficiently requesting policy demonstrations. Third,
    we'll show that a controllable agent can adaptively plan to proactively infer the policy  of the uncontrollable
    agent. The controllable agent will receive a bonus reward when states with a high uncertainty are visited.

    Unlike \cite{lim2013reinforcement} and \cite{bertuccelli2012robust}, we do not limit the case to adversaries; there
    is a chance that the uncontrollable agent will help earn rewards. Finally, we perform the inference with only
    state-sequences. The actions taken by the uncontrolable agent are not observable, only the action-outcome.

