%
% Since this is a ``report'', the topmost level of hierarchy is
% ``Chapter'', not section as you may be used to. Chapters are
% enumerated starting from 1, so Sections are 1.1, Subsections are
% 1.1.1, subsubsections don't get numbers. (You can change that, if
% you want them to be called 1.1.1.1)
%
\chapter{Motivation for Active Policy Inference}\label{chapt:motivation}

%\todo[inline, color=green]{
%                           Throughout this document, I precede each section with a note about the section. That note
%                           should explain the goal of the section, e.g., what I'm intending to convey in the section.
%                           When reviewing, please consider if the section adequately addresses the following points:
%
%                           1) Does the section meet the goal I stated.
%
%                           2) Is this the correct place in the document for that goal to be fulfilled (\eg, is it too
%                              early or too late in the document.
%
%                           3) If you're reading this, I'm already grateful for your help.
%                          }
%
%\todo[inline]{
%              My goal in this section is to provide motivation for the problem I have addressed in this thesis. I
%              introduce a high level example, and then discuss current literature.
%             }

    A fundamental assumption of this thesis is that when an autonomous agent moves in an environment it is attempting to
    fulfill a specified task. The agent could be an autonomous vehicle or a robotic manipulator and in a realistic
    environment there will be static obstacles and other autonomous, uncontrollable, agents. When specifying the main
    task for the controllable agent, there are often requirements such as \quotationMarks{drive from point A to point B
    and neither leave the road nor collide with another car or a pedestrian.} Similarly, a manipulator might be tasked
    to \quotationMarks{pick up everything on the table and put it in the box that the human is holding.} It's clear that
    accurately representing the probabilistic motion distribution of uncontrollable agents -- cars, pedestrians, or
    people holding boxes -- will benefit the autonomous agent as it tries to complete its main objective. This thesis
    details an algorithm that elicits more informative agent interactions that the controllable agent then uses to
    effectively plan its future actions. This work is built upon both recent and fundamental literature in the \ac{RL}
    community.

    \section{Introduction}\label{sec:introduction}

    Modeling stochastic environments with discrete state and action spaces has traditionally been accomplished with
    \iac{MDP}. An agent that takes planned actions in \iac{MDP} is following its \textit{policy}. When an autonomous
    agent is given a task to complete in its environment, the agent is rewarded upon completing this task. In a
    realistic scenario, there are often too many unknowns to solve for a control policy that has the highest likelihood
    or returning a reward. The agent may not know how the actions it can take affect its transitions from one state to
    another, and, therefore, can't immediately make a valuable plan. Or, the agent may know, what actions to
    take to earn the reward, but environmental disturbances require that the agent adaptively plan. In this last case,
    the agent needs to learn a model of its environment before it can claim that it can \textit{optimally} receive a
    reward from an initial state or configuration.

    In this work, we'll assume that a stochastic environment has been successfully modeled, and that, given an arbitrary
    task, our controllable agent can solve for an optimal policy that maximizes the likelihood of reward. However, when
    we introduce a second, uncontrollable, agent into the environment there are several options for how to re-plan.
    We'll assume that the controllable agent needs to continue performing its original task, therefore it would not
    suffice to spend time relearning an environment model or optimal policy that would incorporate the new, mobile
    obstacle, in an abstract manner.  Instead, we build upon \textit{policy gradient} methods to infer the policy of the
    uncontrolable agent, and subsequently converge to the optimal policy for the controllable agent, over the course of
    many sets of interactions. To accelerate this process, we add a novel feedback path for the uncertainty in the
    inferred parameters, which guides exploration.

    In Sections \ref{sec:literature_hipmdp}-\ref{sec:literature_novelty} we'll cover the how current models and
    algorithms built the foundation for this active policy inference algorithm. Section \ref{} will cover common
    notation in \ac{RL} and \ac{MDP} to serve as preliminary information to the main result in Chapter
    \ref{chapt:gauss_policy}. The following chapter, \colorbox{yellow}{\ref{chapt:experiments}}, will show two
    experiments that test the active policy inference.


    For the rest of Chapter \ref{chapt:motivation}, a good example to keep in mind is:
    \begin{problem}\label{prob:factory_example}
        Each day, a mobile robot repeatedly traverses a factory floor to deliver a parcel. Eventually, a second robot
        also starts working in the area, and the first robot does not know anything about the second robot's task. Now,
        the first robot must not collide with the second robot, and without knowledge of the second robot's intent, it
        now takes longer to deliver it's parcel each day. After each week, the first robot can it's observations of the
        inter-robot interactions to improve its delivery plan. Is there a better solution to greedily updating the plan?
        Can the first robot plan to move in such a way that will expose more about the second robot?
    \end{problem}

\section{Inferring Latent Variables in MDPs}\label{sec:literature_hipmdp}

        To solve an adaptive \ac{MDP} \cite{hernandez2012adaptive}, the agent must learn or infer latent (hidden)
        environmental parameters. The main techniques to accomplish this fall into two distinct categories,
        model-\textit{based} \ac{RL}, and model-\textit{free} \ac{RL}. Imitation learning, as well as active learning,
        also share algorithmic traits that the algorithm presented here will mimic. In general, the \ac{RL} comunity is
        now focusing on methods that combine model-free with model-based approaches. A recent survey of these algorithms
        is \cite{polydoros2017survey}.

\subsection{Model Based Solutions}\label{sec:model_based_lit}

    Given that there are some unknown parameters in the multi-agent scenario described in Problem
    \ref{prob:factory_example}, we'll extend the model from \iac{MDP} to a \ac{HiPMDP} as introduced in
    \cite{doshi2016hidden}. \ac{HiPMDP}s are well suited to problems the number of parameters to learn is small relative
    to the size of the state-action space. This is a very applicable framework when certain aspects of the environment
    are already known, and sensory information is precise and accurate.

    Proprioceptive sensors and the agent's actuators have intrinsic biases and noise. Exteroceptive sensors are
    additionally subject to occlusion. The \ac{POMDP} frames these problems and uses a Bayesian update given observed
    state transitions to adjust the beliefs of the current state and parameterization of a transition model
    \cite{kaelbling1998planning}. There are many algorithms tuned to solve \acp{POMDP}, as discussed by
    \cite{bandyopadhyay2013intention}, but the model uncertainties scale with the problem's size.

    Arguably, simply exploiting information from a multi-agent interaction only requires a controllable agent to capture
    the \textit{intent} of an uncontrollable agent. One option has been presented in \cite{bandyopadhyay2013intention}
    and uses an instantiation of a \ac{POMDP}, the \iac{MOMDP}, to provide a belief of pedestrians intent when they
    interact with an autonomous golf-cart.  The authors state that \quotationMarks{the [golf-cart]'s ultimate goal is to
    complete the specified task and not to recognize intention.} In other words, learning the policy of an
    uncontrollable agent provides excessive information to handle a vehicle-pedestrian encounter in a crosswalk.


    To prove that a transition model has been learned,  \cite{Fu-RSS-14} puts guarantees on the samples required to
    achieve a \ac{pacmdp}. The authors show that the number of samples needed to learn a transition system in an
    \ac{MDP} that has a bounded model error scales polynomially.  The authors also present an \ac{MLE} of the transition
    model parameters, complete with empirical mean and variance, but this requires a parameter set that scales with the
    size of the state and action domains.

    The \Rmax algorithm from \cite{brafman2002r} is specifically geared to solve a multi-agent Problem
    \ref{prob:factory_example} like. But, \Rmax would require that the controllable agent coerce the visits each
    \textit{joint}-states enough times to conclude, with a specified confidence, what the policy of the uncontrollable
    agent is.

    Using another approach in the face of uncertain transitions, \cite{bertuccelli2012robust} merge adaptive and robust
    methods for solving \acp{MDP}. Normally a robust solution uses the \textit{minimax} approach, it plans for the
    worst-case transition. Instead, \iac{uav} models its uncertain transitions with Dirichlet Distributions. In an
    algorithm like an \ac{ekf}, transition samples form an empirical covariance matrix of the system's transition model
    so that the \ac{uav} can reduce its exposure to a failed mission.

    Also with a minimax objective, \cite{chinchali2017multi} attempt to learn the true model of several adversaries by
    minimizing \quotationMarks{the cost of information gain}. Each adversarial agent is modeled with an \ac{MDP} and a
    set of pre-designed temporal logic specifications. The goal is to identify which specification each adversary is
    trying to satisfy. The controllable agent recieves a reward by maximizing the information gain, the entropy of the
    current state belief.

    Similarly, \cite{lim2013reinforcement} assume that an adversary in \iac{MDP} may have a non-stationary policy. If
    the adversary chooses to play \quotationMarks{nicely}, then a minimax policy would be far too conservative. Each
    time a state transition is observed, the authors record it and subject each new transition to a
    \quotationMarks{stochasticity check}, essentially trying to detect a change in the adversarial policy using the
    Chernoff bound. Both \cite{lim2013reinforcement} and \cite{bertuccelli2012robust} have tried to overcome the
    conservativeness often encountered in \textit{minimax} policies, but environmental agents can agnostic, or even
    complimentary, to the controllable agent's intent.

    In general, model-based solutions allow the agent to leverage it's belief about future states as it plans. This
    belief is reflected in the \textit{value} of each state, but is very sensitive to model errors.

\subsection{Model Free Solutions}
    Model-free methods do not require the agent to learn the underlying transition model, simply the best action at each
    state. In \ac{RL} problems, policy gradient methods sample the policy-space of an agent, trying to maximize the
    likelihood of a reward by iteratively updating parameters that build a policy.

    Traditionally, the \reinforce algorithm \cite{williams1992simple} has a high variability on the gradient used to
    update the parameters. This is especially true when the reward-space is sparse and the system receives a binary
    reward for very small subset of the policy-space. Each sample from the policy-space must be executed on the real
    system, gathering a set of samples. This is referred to as a policy \text{roll-out}, and combining the trajectories
    formed by each roll-out allows the model-free algorithms to make use of Monte-Carlo methods. As the policy
    parameterization is updated, new data must be gathered to ensure statistical independence.

    To reduce the variability of the gradients, \cite{peters2008reinforcement} formalized the practices of using
    \textit{baseline-subtraction} and \textit{natural} gradients. By subtracting an unbiased baseline from each policy
    sample, the variance of the gradient is drastically reduced. But, As noted by \cite{tangkaratt2014model}, samples
    used for the baseline must be independent of all samples used for the policy gradient. Natrual gradients promote
    faster convergence by accounting for the covariance of policies. If a policy \textit{almost} earned a reward, and
    was very close in measure to one that did, it is allowed more influence the parameter gradient.

\subsection{Combination Methods}

    While model-free solutions do not require the system to learn the underlying  transition funciton, the model-based
    approaches provide insight into the probability of future states. Model-free solutions often exhibit slow
    convergence and taking enough data leads to excessive wear-and-tear on the mechatronic systems to be learned.
    Model-based solutions, however, require prior knowledge about the systems structure.

    The \textit{actor-critic} method family combines model free, with model based approaches. Originally presented by
    \cite{konda2000actor}, the actor-critic methods use two stages. The actor is responsible for improving the policy
    through trials using a model free approach, and the critic improves the estimated value function by updating the
    environmental model. Examples of this are \textit{natural} actor-critic in \cite{peters2008reinforcement}, and
    \textit{path-consistancy learning} in \cite{nachum2017bridging}.

    The PILCO algorithm, from \cite{deisenroth2011pilco}, claims exceptional sample efficiency as it learns a dynamics
    model of a physical cart-pole swing-up. It applies a policy gradient inference, but incorporates the variance of
    policy parameters into the policy synthesis by modeling the unknown dynamics as \acp{GP} and using \textit{kriging}
    \cite{rasmussen2004gaussian}.

    The \ac{pgpe} algorithm from \cite{sehnke2010parameter} combines latent transition model parameters in the policy
    gradient. This is expanded by \cite{tangkaratt2014model} with a better estimation of the model. The \ac{lscde}
    method applied assumes a linear combination of transition model parameters and feature functions.

    Many of these algorithms have relied on the random-sampling of policy parameters to effectively add entropy to their
    learning process. For instance, \cite{peters2008reinforcement} use a stochastic acceleration policy to control robot
    manipulator servos. In \ac{pgpe}, the exploration derives from the random sampling of policy parameters from a
    distribution. Regularizing the critic's value function with an entropy term, thus putting value on exploration
    \cite{nachum2017bridging}, does help prevent the actor from converging to a suboptimal policy. Entropy
    regularization works in-practice, but like applying random-restarts to a Q-learning algorithm \color{blue}(need a
    citation for this!)\color{black}, these are heuristic approaches that do not \textit{guide} exploration in a
    formulaic approach.

\subsection{Learning from Demonstration}

    The problem we present, could also be solved with  either \textit{inverse}-\ac{RL} or \text{imitation} learning
    techniques.  Generally, these techniques attempt to learn a policy from expert demonstrations, although the
    \quotationMarks{experts} may be following sub-optimal policies.

    Solving the IRL problem often requires an accurate model of the environment to effectively learn the demonstrated
    reward function and compute a policy that performs similar to the demonstration. Recent work in
    \cite{herman2016inverse} attempts to learn the underlying dynamics while simultaneously solving the IRL problem.

    In an imitation learning approach, the \DAGGER algorithm learns to complete a task in a supervised fashion with
    \cite{ross2011reduction}. \DAGGER addressed the issue of sample independence as a learning agent adjusts its policy
    over time.  Given an initial set of trajectories from an expert, the agent queries the expert for more (partial)
    demonstrations at a diminishing rate over time. Through this combination of demonstrated and imitated policies, the
    agent builds up a dataset of trajectories that it repeatedly uses to train its policy.

\subsection{Active Learning}

    \todo[inline]{provide high level overview of active learning}

    Using a semi-MDP,  \cite{andersen2017active} use a policy that is exploration only but non-Markovian.  By clustering
    states to symbols, where symbols can be chained together to represent high level task options, the exploration
    policy chooses actions that maximize the expected information gain.

    Planning to read the following Fri-Sun:
    \begin{itemize}
        \item \cite{khamassi2017active}
        \item \cite{andersson2017deep}
        \item \cite{martinez2007active}
        \item \cite{akiyama2010efficient}
        \item re-read \cite{andersen2017active}
    \end{itemize}
    Will include summaries or contributions of these papers if applicable.


    Gaussian processes might be a good option for generalizing policy dependence on relative position in future work.

\section{Contributions of this work}\label{sec:contributions}

    In Problem \ref{prob:factory_example}, the controllable agent must infer the policy of the uncontrollable agent
    while it repeatedly completes its task. There are many \ac{RL} algorithms that can be adapted to a policy inference
    task, while others serve as inspiration but share common aspects. For our case, we want to strike a balance between
    \cite{bandyopadhyay2013intention} and \cite{herman2016inverse}, learning more than enough to simply improve the
    controllable agent's policy but not the uncontrollable agents reward or value funciton.

    For our experiment, we'll assume that the uncontrollable agent operates in the same environment as the controllable
    agent; it is subject to the same stochastic transition model. We'll formally state later that we require the
    uncontrollable policy to be stationary. This allows us to use the \ac{HiPMDP} as a foundation. The goal of modeling
    with a \ac{HiPMDP} is to learn or infer a set of parameters that describe the transition function of the
    controllable agent so that it can optimally plan to earn rewards. In this work, the parameters will describe the
    policy of the uncontrollable agent, since the actions of the uncontrollable agent affect the joint state of the
    system. Thus, learning the policy of the uncontrollable agent equates to learning a parameter vector in the
    transition model that the controllable agent must know to optimally plan.

    From a very high level, the procedure that we present has an actor-critic flavor. We infer a policy from
    demonstration (effectively part of the transition model), and then use that inference to determine the value of
    joint-states. In this way we can passively improve the policy of the controllable agent. By only updating the policy
    after collecting a batch of interactive trajectories like \cite{doshi2016hidden}, we still achieve
    sample-independence.

    This work adapts the policy gradient formulation of \cite{herman2016inverse} and an uncontrollable policy
    parameterization from \cite{tangkaratt2014model} with an aditional temperature parameter from
    \cite{nachum2017bridging}.  By maximizing the likelihood of a demonstration like \cite{herman2016inverse}, we do not
    have the variance introduced by the reward function, so baseline-subtraction is not necessary.

    To promote the most informative exploration, we introduce a novel Gaussian parameter approximation to use for policy
    inference. We extend a Q-function approximation from \cite{Sugiyama2015StatisticalRL} to be a linear combination of
    normal distributions and features. By infering the first and second moments of these distributions, we propagate the
    parameter variance to create a feedback path to the policy-synthesis step. This formulation has similarities to the
    variance computed in \ac{pgpe} from \cite{tangkaratt2014model} and allows the controllable agent to actively seek
    interaction with the uncontrollable agent. This accelerates the inference procedure and allows the controllable
    agent to eventually compute an optimal plan for its task.


    Unlike, \cite{lim2013reinforcement} and \cite{bertuccelli2012robust}, we do not limit the case to adversaries; there
    is a chance that the uncontrollable agent will help earn rewards. The exploration method, as well as  the
    decomposition of the joint-state-action space into a collection of features, doesn't require the system to
    explicitly visit every joint-state like \Rmax. We transfer the parameter distribution information between batches,
    and, therefore do not require the continual growth of the dataset like \DAGGER. Finally, there also parallels to
    imitation learning. If the controllable agent can request a demonstration from the uncontrollable agent, we can
    algorithmically determine the best request. This is in contrast to the hyper-parameters used by \DAGGER to gradually
    decay the number of demonstrations requests.

    The final point to note is that most \ac{RL} algorithms can record which actions the agent took and observe state
    transitions, assuming the problem is not a \ac{POMDP}. Like \cite{lim2013reinforcement}, our inference is only based
    upon state-sequences, not \textit{state-acation} sequences.

