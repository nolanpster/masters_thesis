%
% Since this is a ``report'', the topmost level of hierarchy is
% ``Chapter'', not section as you may be used to. Chapters are
% enumerated starting from 1, so Sections are 1.1, Subsections are
% 1.1.1, subsubsections don't get numbers. (You can change that, if
% you want them to be called 1.1.1.1)
%
\chapter{Motivation for Active Policy Inference}\label{chapt:motivation}

%\todo[inline, color=green]{
%                           Throughout this document, I precede each section with a note about the section. That note
%                           should explain the goal of the section, e.g., what I'm intending to convey in the section.
%                           When reviewing, please consider if the section adequately addresses the following points:
%
%                           1) Does the section meet the goal I stated.
%
%                           2) Is this the correct place in the document for that goal to be fulfilled (\eg, is it too
%                              early or too late in the document.
%
%                           3) If you're reading this, I'm already grateful for your help.
%                          }
%
%\todo[inline]{
%              My goal in this section is to provide motivation for the problem I have addressed in this thesis. I
%              introduce a high level example, and then discuss current literature.
%             }

    A fundamental assumption of this thesis is that when an autonomous agent moves in an environment it is attempting to
    fulfill a specified task. The agent could be an autonomous vehicle or a robotic manipulator and in a realistic
    environment there will be static obstacles and other autonomous, uncontrollable, agents. When specifying the main
    task for the controllable agent, there are often requirements such as \quotationMarks{drive from point A to point B
    and neither leave the road nor collide with another car or a pedestrian.} Similarly, a manipulator might be tasked
    to \quotationMarks{pick up everything on the table and put it in the box that the human is holding.} It's clear that
    accurately representing the probabilistic motion distribution of uncontrollable agents -- cars, pedestrians, or
    people holding boxes -- will benefit the autonomous agent as it tries to complete its main objective. This thesis
    details an algorithm that elicit more informative agent interactions that the controllable agent then uses to
    effectively plan its future actions. This work is built upon both recent and fundamental literature in the \ac{RL}
    community.

    Modeling stochastic environments with discrete state and action spaces has traditionally been accomplished with
    \iac{MDP}. An agent that takes planned actions in \iac{MDP} is following its \textit{policy}. When an autonomous
    agent is given a task to complete in its environment, the agent is rewarded upon completing this task. In a
    realistic scenario, there are often too many unknowns to solve for a control policy that has the highest likelihood
    or returning a reward. The agent may not know how the actions it can take affect its transitions from one state to
    another, and, therefore, can't immediately make a valuable plan. Or, the agent may know, nominally, what actions to
    take to earn the reward, but the environment in which the agent acts may only allow for stochastic transitions
    between states; in this case the agent needs to learn a model of its environment before it can claim that it can
    \textit{optimally} receive a reward from an initial state or configuration.

    In this work, we'll assume that a stochastic environment has been successfully modeled, and that, given an arbitrary
    task, our controllable agent can solve for an optimal policy that maximizes the likelihood of reward. However, when
    we introduce a second, uncontrollable, agent into the environment there are several options for how to re-plan.
    We'll assume that the controllable agent needs to continue performing its original task, therefore it would not
    suffice to spend time relearning an environment model or optimal policy that would incorporate the new, mobile
    obstacle, in an abstract manner.  Instead, we build upon \textit{policy gradient} methods to infer the policy of the
    uncontrolable agent, and subsequently converge to the optimal policy for the controllable agent, over the course of
    many sets of interactions. To accelerate this process, we add a novel feedback path for the uncertainty in the
    inferred parameters, which guides exploration.

    In Sections \ref{sec:literature_hipmdp}-\ref{sec:literature_novelty} we'll cover the how current models and
    algorithms built the foundation for this active policy inference algorithm. Section \ref{} will cover common
    notation in \ac{RL} and \ac{MDP} to serve as preliminary information to the main result in Chapter
    \ref{chapt:gauss_policy}. The following chapter, \colorbox{yellow}{\ref{chapt:experiments}}, will show two
    experiments that test the active policy inference.


    For the rest of Chapter \ref{chapt:motivation}, a good example to keep in mind is:
    \begin{problem}
        A mobile robot repeatedly traverses a factory floor to deliver a parcel. At some-point, a another robot also
        starts working in the area, and the robot does not know what the other robot ................is this excessive
        or is this a good place to put an example problem for the reader? Thoughts?
    \end{problem}

\section{Latent Variables in Markov Decision Processes}\label{sec:literature_hipmdp}

    Given that there are some unknown parameters in the multi-agent scenario described above, we'll extend the model
    from \iac{MDP} to a \ac{HiPMDP} as introduced in \cite{doshi2016hidden}. When a small subset of parameters nead to
    be learned in comparison to the size of the state-action space, \ac{HiPMDP}s are well suited.
    %This is especially true if a previously learned model can be transferred to another agent or task %to kick-start
    %its policy search \colorbox{yellow}{\cite{killian2017robust}}.
    This is very applicable when certain aspects of the environment are already known, and sensory information is
    precise and accurate. For our experiment, we'll assume that the uncontrollable agent operates in the same
    environment as the controllable agent; it is subject to the same stochastic transition model.

    The goal of modeling with a \ac{HiPMDP} is to learn or infer a parameter \paramVec\ that describes the transition
    function of the controllable agent so that it can optimally plan to earn rewards. In this work, that parameter will
    describe the policy of the uncontrollable agent, since the actions of the uncontrollable agent affect the joint
    state of the system. Thus, learning the policy of the uncontrollable agent equates to learning a parameter vector in
    the transition model that the controllable agent must know to optimally plan.

    In more general problems,  sensors and actuators have intrinsic biases and noise, and are also subject to extrinsic
    errors such as occlusion. The \ac{POMDP} is the framework used for these problems and was introduced in
    \cite{kaelbling1998planning}. The \ac{POMDP} model uses a Bayesian update given observed state transitions to update
    the belief of the current state, as well as a parameterization of a transition model.

    Arguably in our experiment we only need to capture the \textit{intent} of the uncontrollable agent. One option has
    been presented in \cite{bandyopadhyay2013intention}, which used \iac{MOMDP} to provide a belief of pedestrians
    intent when they interact with an autonomous golf-cart.  The authors state that ``the [golf-cart]'s ultimate goal is
    to complete the specified task and not to recognize intention." In other words, learning the policy of an
    uncontrollable agent provides excessive information to handle a vehicle-pedestrian encounter in a crosswalk.
    Instead, we'll opt to infer a complete approximation of the uncontrollable, stochastic, policy so that the
    controllable agent can improve its planning over time.

    Model uncertainty in \ac{MDP}s can be dealt with in several ways. It's possible to explicitly learn an underlying
    transition model like \cite{Fu-RSS-14}, which introduces the \ac{pacmdp}. Here, the authors show a polynomial bound
    on the number of samples needed to approximately learn a transition system in an MDP within a confidence interval.
    The authors also present an \ac{MLE} of the transition model parameters, complete with empirical mean and variance,
    but this requires a parameter set that scales with the size of the state and action domains. Similarly, the \Rmax
    algorithm from \cite{brafman2002r} would require that the controllable agent coerce the multi-agent system to visit
    states until each \textit{joint} state has been visited enough times to conclude, with a specified confidence, what
    the policy of the uncontrollable agent is. Instead, we'll generalize our inference to use a feature set and won't
    require the system to visit explicitly every joint-state.

    Using another approach in the face of uncertain transitions, \cite{bertuccelli2012robust} merge adaptive and robust
    methods for solving \acp{MDP}. Normally a robust solution uses the \textit{minimax} approach, it plans for the
    worst-case transition. Instead, in \cite{bertuccelli2012robust}, \iac{uav} models its uncertain transitions with a
    Dirichlet Distribution. In an algorithm like an \ac{ekf}, transition samples form an empirical covariance matrix of
    the systems transition model so that the \ac{uav} can reduce its exposure to a failed mission.

    Also with a minimax objective, \cite{chinchali2017multi} attempt to learn the true model of several adversaries by
    minimizing \quotationMarks{the cost of information gain}. Each adversarial agent is modeled with an \ac{MDP} and
    generalized temporal logic specification that they are presumably trying to satisfy. The information gain is
    rewarded by reducing the entropy of belief of the current state. Each temporal logic specification is designed ahead
    of time, and the agent interacts with its environment to identify which specification the adversary is following.

    Similarly, \cite{lim2013reinforcement} assume that an adversary in \iac{MDP} may have a non-stationary policy. If
    the adversary chooses to play \quotationMarks{nicely}, then a minimax policy would be far too conservative. Each
    time a state transition is observed, the authors record it and subject each new transition to a
    \quotationMarks{stochasticity check}, essentially trying to detect a change in the adversarial policy using the
    Chernoff bound. Both \cite{lim2013reinforcement} and \cite{bertuccelli2012robust} have tried to overcome the
    conservativeness often encountered in \textit{minimax} policies. We, however, do not limit the case to adversaries,
    there is a chance that the uncontrollable agent will help earn rewards. We'll formally state later that we require
    the uncontrollable policy to be stationary.

\subsection{Comparison to Reinforcement Learning}
    The controllable agent must be able to infer the policy of the uncontrollable agent while it repeatedly completes
    its task. There are many \ac{RL} algorithms that can be adapted to a policy inference task, while others serve as
    inspiration but share common aspects. This work adapts the \ac{pgpe} algorithm presented, and extended, by
    \cite{sehnke2010parameter} and \cite{tangkaratt2014model}, respectively. In \ac{RL} problems, policy gradient
    methods search the policy-space of an agent, trying to maximize the likelihood of a reward by iteratively updating
    parameters that build a policy. Traditionally, the \reinforce algorithm from \cite{williams1992simple} has a high
    variability on the gradient used to update the parameters, especially when the reward-space is sparse. For instance,
    a binary reward may only be awarded for policies in a very, very, small subset of the policy-space.

    To reduce the variability of the gradients, \cite{peters2008reinforcement} formalized the practices of using
    \textit{baseline-subtraction} and \textit{natural} gradients. By subtracting an unbiased baseline from each policy
    sample, the variance of the gradient is drastically reduced and natrual gradients promote faster convergence.

    The advantage of policy-gradient methods is that they are traditionally \textit{model-free}; an underlying
    transition model of an \ac{MDP} will not be learned. These methods also do not require the system to simultaneously
    learn the optimal value-function which can be computationally, and sampling-inefficient. The sampling-inefficiency
    is two fold, since convergence is slower and it provides more wear-and-tear on an mechatronic system.

    The \textit{actor-critic} method family combines model free approaches, with model based approaches as discussed in
    Sec. \ref{sec:literature_hipmdp}. Originally presented by \cite{konda2000actor}, the actor-critic methods use two
    stages. The actor is responsible for improving the policy through trials, and the critic improves the estimated
    value function. Examples of this are \textit{natural} actor-critic in \cite{peters2008reinforcement}, and
    \textit{path-consistancy learning} in \cite{nachum2017bridging}.

    The PILCO algorithm from \cite{deisenroth2011pilco}, claimes exceptional sample efficiency as it learns a dynamics
    model of a physical cart-pole swing-up. It applies a policy gradient inference, but incorporates the variance of
    policy parameters into the policy synthesis, and modeling the dynamics as Gaussian Proccesses.

    Many of these algorithms have relied on the random-sampling of policy parameters to effectively add entropy to their
    learning process. For instance, \cite{peters2008reinforcement} use a stochastic acceleration policy to control robot
    manipulator servos. Regularizing the critic's value function with an entropy term, thus putting value on
    exploration, \cite{nachum2017bridging} does help prevent the actor from converging to a suboptimal policy.

    Our framework applies a an adapted \ac{pgpe} framework, using the policy gradient method from
    \cite{tangkaratt2014model} and \cite{Sugiyama2015StatisticalRL}. By maximizing the likelihood of a demonstration
    like \cite{herman2016inverse}, we do not have the variance introduced by the reward function, so
    baseline-subtraction is not necessary. From a very high level, the procedure that we present has an actor-critic
    flavor. We infer a policy from demonstration, and then use that inference to determine the value of joint-states. In
    this way we can passively improve the policy of the controllable agent.

    Finally, for a recent survey of \ac{RL} algorithms, see \cite{polydoros2017survey}.

\subsection{Imitation Learning}

    The problem we present, could also be solved with  \textit{inverse}-\ac{RL} or \text{imitation} learning techniques.
    Generally, these techniques attempt to learn a policy from expert demonstrations, although the
    \quotationMarks{experts} may be following sub-optimal policies.

    Solving the IRL problem often requires an accurate model of the environment to effectively learn the demonstrated
    reward function and compute a similar performing policy. Recent work in \cite{herman2016inverse} attempts to learn
    the underlying dynamics while simultaneously solving the IRL problem. For our case, we want to strike a balance
    between \cite{bandyopadhyay2013intention} and \cite{herman2016inverse}, learning more than enough to simply improve
    the controllable agents chance at completing the task but not the uncontrollable agents reward or value funciton.

    In an imitation learning approach, the \DAGGER algorithm learns to complete a task in a supervised fashion with
    \cite{ross2011reduction}. \DAGGER addressed the issue of sample independence as a learning agent adjusts its policy
    over time.  Given an initial set of trajectories from an expert, the agent queries for expert for more (partial)
    demonstrations at a diminishing rate over time, and builds up a dataset of trajectories that it repeatedly uses to
    train its policy.

\subsection{Active Learning}

    Using a semi-MDP,  \cite{andersen2017active} use a policy that is exploration only but non-Markovian.  By clustering
    states to symbols, where symbols can be chained together to represent high level task options, the exploration
    policy chooses actions that maximize the expected information gain.


\subsection{Novelty of this thesis (better-ize this name)}\label{sec:literature_novelty}

    \begin{itemize}
        \item only observing state-action outcomes similar to \cite{renato} and \cite{lim2013reinforcement}
            (state-sequences), not state-action-sequences.

        \item less conservative than mini-max, we don't rule out the possiblility that the other agent is actually
            helping us complete the main objective.
        \item use novel Gaussian policy approximation for policy inference
        \item (\textit{hopefully}) provide an exploration bonus reward that accelerates data gathering for the best
            inference.
        \item We parameterize the policy using a set of basis functions that reduce the dimensionality of the required
            inference from $|S|\times|A|$ to $|K|\times|A|$ where $|K|$ is the number of kernels used. Each action at
            each kernel receives a basis function.
        \item On stationarity: In the batch framework, the robot policy is fixed for each batch, making those sampled
            trajectories i.i.d.. If, the environmental policy was allowed to change between each batch, then the mean
            and variances of the parameters would adjust accordingly, allowing this algorithm to transcend the bounds we
            have initially given it.

        \item We extend a Q-function approximation that is a linear combination of normal distributions and learn the
            first and second moments of these distributions, from \cite{tangkaratt2014model}. The extension is that we
            feedback the second moment of each distribution to induce exploration and improve the data used for the next
            mini-batch of trajectories.
         \item Generally \ac{HiPMDP}s assume some prior distribution of the parameters. There is a trade-off between too many parameters increasing computational costs, and the generalization error induced by too few parameters. By propagating the second-moment of each parameter distribution we can more effectively identify extraneous parameters, as well as parameters that are unknown and may, or may not, be necessary.
         \item we improve over \Rmax by not valuing every exploration step equally, but preferentially exploring some more informative states.

         \item \cite{doshi2016hidden} uses a batch-inference, we use mini-batch?

         \item There are also parallels to imitation learning, but we now have a feedback path that determines the states where an
         agent would need assistance, instead of the heuristic methods used in DAGGER.

         \item Many implementations of \ac{pgpe} parameterize the policy as drawn from a normal distribution, with a mean value

    \end{itemize}

\subsection{Preliminaries}\label{sec:preliminaries}
Because our experiment requires resolving for a policy, we'll use the \ac{EM} solution from
\cite{toussaint2010expectation}. The computational requirements to solve for a sub-optimal policy for \agent{1} are
much less than traditional value iteration. Although the solution is sub-optimal, the solution is complete for the
entire state-action-space. The resulting policy is stochastic, which can help gather more data.
