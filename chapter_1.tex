%
% Since this is a ``report'', the topmost level of hierarchy is
% ``Chapter'', not section as you may be used to. Chapters are
% enumerated starting from 1, so Sections are 1.1, Subsections are
% 1.1.1, subsubsections don't get numbers. (You can change that, if
% you want them to be called 1.1.1.1)
%
\chapter{Motivation for Proactive Policy Inference}\label{chapt:motivation}

%\todo[inline, color=green]{
%                           Throughout this document, I precede each section with a note about the section. That note
%                           should explain the goal of the section, e.g., what I'm intending to convey in the section.
%                           When reviewing, please consider if the section adequately addresses the following points:
%
%                           1) Does the section meet the goal I stated.
%
%                           2) Is this the correct place in the document for that goal to be fulfilled (\eg, is it too
%                              early or too late in the document.
%
%                           3) If you're reading this, I'm already grateful for your help.
%                          }
%
%\todo[inline]{
%              My goal in this section is to provide motivation for the problem I have addressed in this thesis. I
%              introduce a high level example, and then discuss current literature.
%             }

    A fundamental assumption of this thesis is that when an autonomous agent moves in an environment it is attempting to
    fulfill a specified task. The agent could be an autonomous vehicle or a robotic manipulator and in a realistic
    environment there will be static obstacles and other autonomous, uncontrollable, agents. When specifying the main
    task for the controllable agent, there are often requirements such as \quotationMarks{drive from point A to point B
    and neither leave the road nor collide with another car or a pedestrian.} Similarly, a manipulator might be tasked
    to \quotationMarks{pick up everything on the table and put it in the box that the human is holding.} It's clear that
    accurately representing the probabilistic motion distribution of uncontrollable agents -- cars, pedestrians, or people holding boxes -- will
    benefit the autonomous agent as it tries to complete its main objective. This thesis details algorithms that elicit
    the most informative agent interactions that the controllable agent then uses to effectively plan its future
    actions. This work is built upon both recent and fundamental literature in the \ac{RL} community.

\todo[inline]{make all graphics '.tif' format for highest quality}

\section{Current Literature}
    \todo[inline]{copy literature reviews from my other documents}
    ** Look through Prof. Fu's work as well as previously cited RL-Books, and papers.

\subsection{Parameter Uncertainty in MDPs}
    Literature review of papers detailing parameter estimation in MDPs here.

\subsection{Policy Inference from Observation}
    Literature review of papers about policy inference in RL.

\subsection{Active Learning}
    Literature review of papers about active learning.

\subsection{Novelty of this thesis (better-ize this name)}
    \todo[inline]{Rewrite as fluid paragraph}
    \begin{itemize}
        \item only observing state-action outcomes, (state-sequences) not state-action-sequences.
        \item less conservative than mini-max, we don't rule out the possiblility that the other agent is actually
            helping us complete the main objective.
        \item use novel Gaussian policy approximation for policy inference
        \item (\textit{hopefully}) provide an exploration bonus reward that accelerates data gathering for the best
            inference.
        \item We parameterize the policy using a set of basis functions that reduce the dimensionality of the required
            inference from $|S|\times|A|$ to $|K|\times|A|$ where $|K|$ is the number of kernels used. Each action at
            each kernel receives a basis function.
    \end{itemize}
