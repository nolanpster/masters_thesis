%
% Since this is a ``report'', the topmost level of hierarchy is
% ``Chapter'', not section as you may be used to. Chapters are
% enumerated starting from 1, so Sections are 1.1, Subsections are
% 1.1.1, subsubsections don't get numbers. (You can change that, if
% you want them to be called 1.1.1.1)
%
\chapter{Policy Inference with Gaussian Policy Estimate}\label{chapt:gauss_policy}

    The multi-agent system presented in this thesis is modeled with two agents, the controllable agent \agent{1} and the
    uncontrollable agent \agent{2}. The interraction between the two agents is captured by a \acf{MDP}. We assume that
    \agent{2} has a predefined yet unknown policy. If \agent{1} is given a task, it can only robustly plan for this task
    when it has an accurate estimate of how \agent{2} will act. Determining the action distribution of an uncontrollable
    agent given its current state is known as \textit{policy-inference}. The following definitions formally present a
    hidden policy in \iac{MDP}.

    After defining the hidden-parameter MDP, this chapter presents an inference method that uses Monte-Carlo integration
    of policy parameters sampled from a multivariate distribution. We'll use \ac{SGA} to improve the log-likelihood of
    an observed set of data given a policy parameterization. Section \ref{sec:policy_parameterization} details a
    $Q$-function approximation which is used to form a softmax policy like in \cite{nachum2017bridging}, also known as a
    Boltzman \cite{Hanawal2017LearningPolicies} and a Gibbs \cite{Sugiyama2015StatisticalRL} policy.

\section{Hidden-parameter MDP}\label{sec:hipmdp}
    \begin{definition}\label{def:hipmdp}
        The interaction between two agents is captured by a hidden-parameter \ac{MDP},
        \[
            \mathcal{M} = (S, A_1 \times A_2, R, T, \policy{2}, I_0 , \gamma)
        \]
        with the tuple defined as in \cite{Sugiyama2015StatisticalRL} plus the hidden parameter, $\policy{2}$:
        \begin{itemize}
            \item $S \equiv (S_1 \times S_2)$ is a set of joint states with cardinality $0 < |S| < \infty$ .
            \item $A_1\times A_2$ is a finite set of actions, where $A_1$ is the set agent one can execute and $A_2$ is
                the set available to agent two.
            \item $R: S\times A_1 \rightarrow \reals$ is the real-valued state-action reward function given to the
                controllable agent.
            \item $T: S\times (A_1\times A_2)\rightarrow \dist(S)$ is the probabilistic state transition function
                $ T(s'| s, (a_1,a_2)) $ which yields the probability of reaching state $\textnormal{s}'$ after both
                agents take action pair $(\text{a}_1,\text{a}_2)$ at the state $\textnormal{s}$.
            \item $\policy{2} : S \rightarrow \dist(A_2)$ The distribution of agent two's actions given a
                state. The probability of each action is $\policy{2}(a_2|s)$.
            \item $I_0 \in \dist(S)$ is the initial state distribution.
            \item $\gamma \in (0,1]$: The discounting factor.
        \end{itemize}
    \end{definition}

    \noindent
    This definition also leads us to a couple pivotal assumptions in this work:

    \begin{assumption}
        The state-action transition function $T(\cdot)$ is known.
    \end{assumption}

    We'll also need two definitions from \cite{Guo2009} about policies to continue:

    \begin{definition}{Markov Policy:} \label{def:markov_policy}
        A policy $\pi_t(s)$ is considered to be a Markov policy if $P(a_j|h_t) = P(a_j|s^{(N)}), \forall a_j \in A$,
        where $h_t = s^{(0)},...,s^{(N)}$ is a state history ending at some discrete time $t \in [0,\infty)$.
    \end{definition}

    \begin{definition}{Stationary Policy:} \label{def:stationary_policy}
        A policy $\pi_t(s)$ is considered to be a stationary policy if $\pi_t(s_i) = \pi(s_i), \forall s_i \in S$,
        for all time $t \in [0,\infty)$.
    \end{definition}

    Thus, a stationary Markov policy is fixed with respect to time, and is only dependent on the current state of
    the system; it is independent of history.

    \begin{assumption}\label{assump:stationary_markov}
        The unknown policy of \agent{2}, $\policy{2}$ is a stationary Markov policy.
    \end{assumption}

    Given that \agent{1} takes some action $a_1$, the distribution of next states is
    \begin{equation}\label{eq:true_state_action_trans_prob}
        P(s'| s, a_1) = \sum_{a_2\in A_2} T\left(s'|s, (a_1,a_2)\right)\policy{2}(a_2|s).
    \end{equation}

    \noindent
    With this model, the true probability of a future state $s'$ given the current state $s$ is
    \begin{equation}\label{eq:true_state_trans_prob}
        p(s'|s) = \sum_{a_1, a_2 \in A} T \left(s'|s, (a_1,a_2)\right)\policy{2}(a_2|s)\policy{1}(a_1|s),
    \end{equation}

    \noindent
    where, for simplicity, this report uses identical action sets, $A_1 \equiv A_2 \equiv A$. Finally, the \ac{MDP}
    formed by the tuple $(S, A_1 \times A_2, R, T, \policy{2}, I , \gamma)$ will be referred to as $\mathcal{M}$.

\section{Unknown Policy Parameterization}

\subsection{Preliminaries}\label{sec:policy_iteration_preliminaries}
    The parameterization for \policy{2} requires a an understanding of how a reward function, $R$, influences the
    optimal action distribution at a state $s$. Although we do not wish to learn the reward function of \agent{2}, we
    will build approximations of the \textit{value} and \textit{state-action value} functions used by \agent{2}. These
    are often a function of a reward function.

\subsubsection{Value Functions}
    Following \cite{hernandez2012adaptive} and \cite{Sugiyama2015StatisticalRL}, the value of a state $s$ is defined as the
    expectation over all discounted future rewards that could be earned from $s$. For an arbitrary, single-agent \ac{MDP},
    if an agent \agent{i} follows a policy \policy{i}:
    \[
    V_{\policy{i}}(s) = \expectation{\policy{i}}{\sum_{t=0}^{\infty} \gamma^t R\left((s,a_i)^{(t)}\right)
            \bigg|s^{(t)}=s}.
    \]
    The subscript in $\expectation{\policy{i}}{\cdot}$ means that the expectation is taken over the single agent Markov
    chain that is induced by \agent{i} playing policy $\policy{i}$ in the \ac{MDP},
    \[
    V_{\policy{i}} = \expectation{\policy{i}}{R\left((s,a_i)^{(t)}\right) + \gamma V_{\policy{i}}(s^{(t+1)})}.
    \]

\subsubsection{State-action Value (Q) Functions}
    Likewise, the value of an action at a particular state is the expected discounted future reward following policy
    \policy{i}:
    \[
    Q_{\policy{i}}(s,a_i) = \expectation{\policy{i}}{\sum_{t=0}^{\infty} \gamma^t R\left((s,a_i)^{(t)}\right)
            \bigg|s^{(t)}=s, a_i^{(t)}=a_i}.
    \]


\subsection{Q-function Approximation}\label{sec:policy_parameterization}
    The model of $\policy{2}(s; \paramVec)$ approximates the \textit{state-action value function}, $Q(s,a_2)$, with a
    linear-in-parameter model. Sugiyama describes this model in detail in Section 2.2.1 of
    \cite{Sugiyama2015StatisticalRL}, it is also used by \cite{Hanawal2017LearningPolicies}. It is advantageous that
    \agent{1} does not need to learn the reward function of \agent{2}, but only the distribution of \agent{2}'s action
    given the joint state set, $\policy{2}(s),\ s\in S$. Each parameter element determines the weighting of a feature
    $\phi : S \times A_2 \rightarrow \reals$. If a total of $\paramLen$ features are used then \agent{2}'s
    \textit{Q}-function is approximated as
    \begin{equation*}\label{eq:QFuncApprox}
    Q(s,a_2) \approx \estimate{Q}(s,a_2)
    = \sum_{\paramIdx=1}^{\paramLen} \estimate{\paramSym}_{\paramIdx} \featElem(s,a_2)
    = \estimate{\paramVec}^\top \featFunc.
    \end{equation*}

    Following Chapter 3 of \cite{Sugiyama2015StatisticalRL}, we'll use \acp{GGK} because they intuitively incorporate
    any obstacles\footnotemark\ that exist in $S$ into a distance metric between a state $s$, and the center of a
    \ac{GGK}, $c$. Consider the state-graph of $\mathcal{M}$. If there are $K$ kernels, and the $l$-th kernel has center
    $c$, then the value of the kernel at a state $s$ is
    \begin{equation*}
        \kernFunc(s,\kernCent_{\kernIdx}) = \exp \left( \frac{-\text{SP}(s,\kernCent_{\kernIdx})^2}
                                                             {2\kernStdDev^2_{\kernIdx}} \right).
    \end{equation*}

    \footnotetext{Obstacles could be considered as known sink-states in $\mathcal{M}$, and will be clarified in Sect.
                  \ref{sec:single_agent_experiment}.}

    \noindent
    The \ac{SP} between a state $s$ and the $l$-th kernel's center $c_l$ can be precomputed, and the kernel standard
    deviation, $\kernStdDev_{\kernIdx}$, determines the effective support of the kernel. The feature function is defined
    for each action, so the number of features is $W = |A| \times K$. The $w$-th feature function is
    \begin{equation}\label{eq:kernel_func}
        \featElem(s_2,a_2) = I(a_2==a_2^{(j)})\sum_{s_2'\in S_2}P\left(s_2'|s_2,a_2^{(j)}\right)
                                \kernFunc(s_2',c_{\kernIdx}),
    \end{equation}

    \noindent
    where $w=j+(\kernIdx|A_2|-1)$ represents the index used for both feature-vector functions and parameter elements,
    \paramElem. Also, for each action $a_2$ the indicator function ${I(a_2==a_2^{(j)})}$ is defined as:
    \begin{equation*}
        \begin{aligned}
            I(a_2==a_2^{(j)}) & = \begin{cases}
                                    0 & \text{if}\ a_2 \neq a_2^{(j)} \\
                                    1 & \text{if}\ a_2 = a_2^{(j)}
                                  \end{cases} \\
            j & = 1, \ldots, |A_2|,
        \end{aligned}
    \end{equation*}

    \noindent
    assuming all actions are enabled from each state $s_2 \in S_2$.

    Given a $Q$-function, \cite{nachum2017bridging} represents the policy at a state as represented the softmax over all
    actions,
    \begin{equation}\label{eq:policy_model}
        \policy{2}(a_2|s) = \exp((Q(s,a_2)- V(s))/\kappa),
    \end{equation}

    \noindent
    where $V(s) = \kappa\log \sum_{a_2\in A}\exp (Q(s,a)/\kappa)$. Also, $\kappa$ is a temperature parameter. If $\kappa
    \rightarrow \infty$, the distribution $ \policy{2}(s) $ becomes uniform over $A_2$. If $\kappa \rightarrow 0$, the
    distribution $\policy{2}(s)$ becomes a Dirac Delta distribution that peaks on the action $a_2^\ast = \argmax_{a_2}
    Q(s,a_2)$. In the inference algorithm, we consider $\kappa$ as a predefined hyper-parameter in the range
    $(0,\infty)$.

    This is the model used for \policy{2} in the rest of this report. The advantage of this parameterization is that we
    have reduced the number of parameters to learn from $|S|\times A$ to $W$. We'll show that we can effectively infer a
    policy with $K \ll |S|$.

    The form used in Eq. \ref{eq:policy_model} is equivalent to the Gibbs policy from \cite{Sugiyama2015StatisticalRL}.
    It is referred to as a Boltzmann policy in \cite{Hanawal2017LearningPolicies}, and \cite{herman2016inverse},
    although the authors have described this parameterizion without reference to a temperature parameter, in which case
    $\kappa=1$.

\section{Policy Inference}\label{sec:policy_obj}

    For \agent{1} to plan a near-optimal policy for its own task, it must learn the policy of the other agent, which
    gives the model of the MDP. The estimate of \agent{2}'s policy, $\estimate{\policy{}}_2$, is parameterized by a
    vector \vect{\theta}.  Therefore, the estimated probability of a state transition is
    \begin{equation}\label{eq:est_state_trans_prob}
        q(s'|s, \vect{\theta}) = \
            \sum_{a_1, a2 \in A}P(s'|s,(a_1,a_2))\estimate{\policy{}}_2(a_2|s,\vect{\theta})\policy{1}(a_1|s).
    \end{equation}

    As the \agent{2} moves through $S$, the robot agent can observe the outcomes of \agent{2}'s actions, and build a set
    of observed state sequences.
    \begin{definition}\label{def:traj}
        A trajectory $\tau$ is a sequence of joint states $s=(s_1, s_2)$ with time-step index $t$,
        \[
        s^{(0)}, s^{(1)}, \ldots , s^{(t)}, \ldots , s^{(|\tau|)};\ 0 \leq t \leq |\tau|.
        \]
    \end{definition}

    \begin{remark}
        This definition is a key difference from comparable policy gradient algorithms, such as \ac{pgpe}
        \cite{tangkaratt2014model} \cite{sehnke2010parameter}. In lieu of state-action sequences, Section
        \ref{sec:gauss_policy} will present an inference procedure that uses observed state-\textnormal{action-outcome}
        sequences. The action-outcome is assigned to be the most probable \textnormal{(maximum a posteriori)} action
        that can lead from $\textnormal{s}$ to $\textnormal{s'}$ using the known transition function, $T(\cdot)$.
    \end{remark}

    Suppose the policy of agent one, \policy{1} is known. We'll define the probability of a trajectory as the joint
    probability of each set state-transition tuple for each of the two distributions, $p$ and $q$:
    \begin{align*}
        p(\tau) &= \prod_{t=1}^{|\tau|}p\left( s^{(t)}| s^{(t-1)} \right), \\
        q(\tau|\vect{\theta}) &= \prod_{t=1}^{|\tau|}q\left(s^{(t)}| s^{(t-1)}, \vect{\theta}\right).
    \end{align*}

    \noindent
    By using the parameter \paramVec, $q(\tau,\paramVec)$ is the probability of replicating a trajectory given the
    parameterization.

    \begin{assumption}
        The observed demonstration set, D, is sampled i.i.d. from the set of all possible demonstrations $\mathcal{D}$.
    \end{assumption}

    The best inference of an environmental policy has a high likelihood of replicating the trajectories in $D$.
    \begin{lemma}\label{lemma:obj_fun_equiv}
        Minimizing the \ac{KLD} of the replica distribution from the observed trajectory distribution is equivalent to
        maximizing the log-likelihood of the observed state sequences given a parameterized policy. With a fixed policy
        for \agent{1}, $\policy{1}$,
        \begin{equation*}
            \argmax_{\paramVec} \logLike(\paramVec) = \argmin_{\paramVec} \text{KL}(p||q_{\paramVec}).
        \end{equation*}
    \end{lemma}

    \begin{proof}
        Consider a pair of stationary policies for the two agents, $\policy{1}$ and $\policy{2}$. The induced Markov
        chain is $\mathcal{M}_{\policy{1}, \policy{2}}$. Let $p$ be the probability distribution of paths in the chain
        $\mathcal{M}_{\policy{1}, \policy{2}}$. Let $q_{\paramVec}$ be the probability distribution of paths in the
        chain $M_{\policy{1}, \estimate{\policy{}}_2}$. The \ac{KLD} from $q_{\paramVec}$ to $p$ is
        \begin{equation*}\label{eq:traj_kl_div}
            \text{KL}(p || q_{\paramVec}) = \sum_{\traj_d \in D} p(\traj_d) \ln \left( \frac{p(\traj_d)}
                                                {q(\traj_d|\paramVec)} \right)\
                                          = \sum_{\traj_d \in \calD} P(\traj_d|D) \ln \left( \frac{P(\traj_d|\calD)}
                                                {P(\traj_d|\paramVec)} \right),
        \end{equation*}

        \noindent
        where $P(\traj_d|\mathcal{D})$ is the maximum likelihood probability of the state sequence, and $P(\traj_d|\paramVec)$ is
        the probability of obtaining that state sequence by our inferred policy that is parameterized by the vector
        \paramVec.

        Minimizing the deviation of $q_{\paramVec}$ from $p$ is equivalent to maximizing the expectation of the
        observing $D$, given that the environment actions are distributed as $\policy{2}(s; \paramVec)$:
        \begin{equation}\label{eq:min_kld}
            \begin{aligned}
                \argmin_{\paramVec}(\text{KL}(p || q_{\paramVec})) & = \
                    \argmin_{\paramVec}\left(\sum_{\traj_d \in  \calD}\!  P(\traj_d|\calD)
                    \ln\left(\frac{P(\traj_d|\calD)}{P(\traj_d|\policy{1},\paramVec)}\right)\right)\\
                & = \argmin_{\paramVec}\bigg(\underbrace{\sum_{\traj_d \in D} P(\traj_d|D) \ln(P(\traj_d|D))}_{constant} -
                    P(\traj_d|D) \ln\left(P(\traj_d|\policy{1},\paramVec)\right)\bigg)\\
                & = \argmax_{\paramVec}\left(\sum_{\traj_d \in \calD}\!  P(\traj_d|\calD)
                    \ln\left(P(\traj_d|\policy{1},\paramVec)\right)\right)\\
                & = \argmax_{\paramVec}\expectation{P(\traj_d|\calD)} {\ln(P(\traj_d|\policy{1},\paramVec))} \\
                &\approx \argmax_{\paramVec} \sum_{\traj_d \in D}  \ln(P(\traj_d|\policy{1},\paramVec))\\
                & =\argmax_{\paramVec} \logLike(D|\policy{1},\paramVec)
            \end{aligned}
        \end{equation}

        \noindent
        where we estimate the expectation using the empirical mean. We will write the final line of Eq.
        \ref{eq:min_kld} as $\logLike(\paramVec; \policy{1})$ for compactness and consistency with Lemma
        \ref{lemma:obj_fun_equiv}.

    \end{proof}

    For the rest of this report, lets assert that an optimal parameter exists.
    \begin{assumption}\label{assump:opt_policy_err}
        There exists an optimal parameter vector that can represent the true distribution of $\policy{2}(s)$ to within a
        threshold $\xi$, given that a set of basis functions are properly defined;
        \[
            \exists\ \optimal{\paramVec}\ \Big|\  \OneNorm{\estimate{\policy{}}_2(s;\optimal{\paramVec}),\policy{2}(s)}
                \leq \xi.
        \]
    \end{assumption}

    \noindent
    The infinite $\mathsf{L_{\infty}}$-norm between two policies,
    \[
        \OneNorm{\policy{x}(s),\policy{y}(s)} = \sum_{s \in S}\sum_{a \in A}\policy{x}(a|s)-\policy{y}(a|s),
    \]
    is a measurable distance unlike \ac{KLD}; $\text{KL}(p||q) \neq \text{KL}(q||p)$. For all following experiments,
    we'll use the $\mathsf{L_{\infty}}$-norm to compare two policies.

    We are now ready to discuss the inference procedure used to identify the best estimate of $\policy{2}(s, \paramVec)$
    that maximizes the R.H.S of Lemma \ref{lemma:obj_fun_equiv}.


\subsection{Gaussian Distribution of Policy Parameters}\label{sec:gauss_policy}

    After a data set of trajectories, $D$, has been collected, \agent{1} needs to maximize $\logLike(D|\paramVec)$, the
    log-likelihood of the dataset when \policy{2} is parameterized by an estimated parameter vector
    \estimate{\paramVec}. Let $\estimate{\paramVec} = [ \paramElem]_{w=1}^W$ be a vector of independently sampled random
    variables with Gaussian distributions $\mathcal{N}(\mu_w, \nu_w^2)$ for $w=1,\ldots,W$. The variance, $\nu_w^2$,
    will capture the uncertainty of $\paramElem$ in the inference from dataset $D$.

    The following is similar to the analysis in \cite{tangkaratt2014model}, \cite{herman2016inverse}, and
    \cite{sehnke2010parameter} except that we do not include a reward function because the policy sought must replicate
    the observed data $D$, not earn a reward.

    We denote $\rho_w = (\mu_w, \nu_w)$ as the tuple of mean and variance for $\paramElem$ and denote $\rho
    =\{\rho_w\}_{w=1}^W$ to be the collection of variable tuples. Given $\rho$, the probability of the demonstrations is
    \[
        P(D |\rho) = \int_{\paramVec } P(D|\paramVec) p(\paramVec | \rho)d\paramVec.
    \]

    \noindent
     The log-likelihood of the demonstrations can be lower-bounded using Jensen's inequality:
    \begin{equation*}
    \logLike(D|\rho ) = \log P(D|\rho)\
         = \log \left( \int_{\paramVec}P(D|\paramVec) p(\paramVec | \rho)d\paramVec \right)\ \ge \int_{\paramVec}
            p(\paramVec|\rho)\log \big( P(D|\paramVec) \big) d\paramVec.
    \end{equation*}

    \par
    Denote this lower bound as $\tilde{\logLike}(D|\rho)=\int_{\paramVec} p(\paramVec |\rho) \log P(D|\paramVec)
    d\paramVec$. This is the lower bound on the objective function derived in Eq. \ref{eq:min_kld}. By taking
    derivative of $\tilde{\logLike}(D|\rho)$ with respect to $\rho$, we obtain the gradient of the objective function:
    \begin{equation}\label{eq:gradLogLike}
        \begin{aligned}
            \nabla_\rho \tilde{\logLike}(D|\rho)) & =
                \int_{\paramVec}\nabla_\rho p(\paramVec|\rho) \log P(D|\paramVec)d\paramVec\\
            & = \int_{\paramVec}[ p(\paramVec|\rho) \nabla_\rho \log p(\paramVec|\rho) ] \log P(D|\paramVec)d\paramVec\\
            &\approx \frac{1}{m} \sum_{ i=1}^m \left[\nabla_\rho \log P(\paramVec^{(i)}|\rho) \right] \log
                P(D|\paramVec^{(i)})
        \end{aligned}
    \end{equation}

    \noindent
    where $\paramVec^{(i)}$, $i=1,\ldots, m$ are samples generated from the multi-variant Gaussian distribution with
    mean $\vect{\mu} = [\mu_1,\ldots, \mu_W]^\top$ and covariance matrix $\mbox{diag}\left(\nu_1,\ldots, \nu_W\right)$.
    Let $\vect{\nu}$ be an equivalent representation for $\mbox{diag}\left(\nu_1,\ldots, \nu_W\right)$. Each sampled
    parameter element, $\paramElem^{(i)}$, has probability:
    \[
        P(\paramElem^{(i)} | \rho_{\paramIdx}) = \frac{1}{\sqrt[]{2\pi \sigma_w^2}}\exp
            \left( -\frac{(\theta_w^{(i)}-\mu_w)^2}{2\sigma_w^2} \right).
    \]

    \par
    The bracketed gradient in the last line of Eq. \ref{eq:gradLogLike} with respect to each element of \vect{\mu} and
    $\vect{\nu}$ are:
    \[
    \nabla_{\mu_{\paramIdx}}\log P(\paramElem^{(i)} | \rho_{\paramIdx}) =
        \frac{\paramElem^{(i)} - \mu_{\paramIdx}}{\nu_{\paramIdx}^2},\ \text{and}
    \]
    \[
    \nabla_{\nu_{\paramIdx}}\log P(\paramElem^{(i)} | \rho_{\paramIdx}) = \frac{(\paramElem^{(i)} - \mu_{\paramIdx})^2 -
        \nu_{\paramIdx}^2}{\nu_{\paramIdx}^3}.
    \]
    Note that superscripts enclosed in parenthesis represent sample indexes, e.g., the $i$-th sample of parameter
    element $w$ is $\paramElem^{(i)}$. All purely numeric superscripts are exponents.

    We can obtain the optimal collection of parameters $\optimal{\rho}= \argmax_{\rho} \tilde{\logLike{}}(D|\rho)$ by
    performing gradient ascent on the parameter distributions, $\rho = (\vect{\mu}, \vect{\nu})$. The policy
    parameterized by $\optimal{\estimate{\paramVec}} \sim \mathcal{N}(\optimal{\vect{\mu}}, \optimal{\vect{\nu}})$ is
    $\estimate{\policy{}}_{2}(s; \optimal{\estimate{\paramVec}})$ and it maximizes the log likelihood of the
    demonstration set $D$. The log likelihood of observed demonstrations for a given $\paramVec^{(i)}$ can be computed
    as
    \begin{align}\label{eq:log_prob_demo}
        \log P(D|\paramVec^{(i)}) & = \sum_{\tau_d \in D} \log P\left(\tau_d |\paramVec^{(i)}\right)\\
        & = \sum_{d=1}^{\abs{D}} \left[ \sum_{t=0}^{\abs{\tau_d}-1} \log P \left(s^{(t+1)}|(s, a_1, o_2)^{(t)}\right) +
            \sum_{t=0}^{\abs{\tau_d}-1} \log \policy{2}\left(o_2^{(t)}|s^{(t)}; \paramVec^{(i)}\right) \right]\\
        & = \sum_{s\in S}\sum_{o_2\in A} C(s, o_2) \log \policy{2}\left(o_2|s; \paramVec^{(i)}\right) + Const.
    \end{align}

    Above, $C(s, o_2)$ is the number of times the state action pair $(s,o_2)$ is observed from in $D$. The constant
    term, $Const =\sum_{d=1}^{\abs{D}} \sum_{t=0}^{\abs{\tau_d}-1} \log P\left(s^{t+1}|(s ,a_1,o_2)^{(t)}\right)$, is
    independent of $\paramVec^{(i)}$ and can be precomputed for a demonstration $D$. The observed action outcome at
    time-step $t$ is $o_2^t$, which is the only action information available in a trajectory, per Definition
    \ref{def:traj}. If a trajectory fragment $(s_1, s_2)^{(t)}, (s_1', s_2')^{(t+1)}$ is observed, the action of the
    controllable agent, $a_1^{(t)}$, is known but the uncontrolled action, $a_2^{(t)}$, is not. Therefore $o_2^{(t)}$ is
    assigned to be the nominal motion that causes the transition $s_2^{(t)} \rightarrow s_2^{(t+1)}$ in the graph of
    $\mathcal{M}$.

    \begin{remark}
        If the policy does not depend on a basis function $\phi_w$, $\mu_w=0$ and $\nu_w \ll 1$, the basis function can
        be removed from the parameterization since it has no influence on the inferred policy.
    \end{remark}

\subsubsection{Gradient Ascent}

    Using the gradient defined in Eq. \ref{eq:gradLogLike}, for each iteration $n$ we sample a set of $m$ parameter
    vectors, and update the distribution parameters on each iteration:
    \begin{equation}\label{eq:gradient_update}
        \begin{aligned}
            \dot{\vect{\mu}}_n & \leftarrow \eta\dot{\vect{\mu}}_{n-1} \lambda\nabla_{\vect{\mu}_n}
                                    \tilde{\logLike}(D|\rho_n)\\
            \vect{\mu}_{n+1} & \leftarrow\ \vect{\mu}_n + \dot{\vect{\mu}}_n\ \text{and}\\
            \dot{\vect{\nu}}_n & \leftarrow \eta\dot{\vect{\nu}}_{n-1} \lambda\nabla_{\vect{\nu}_n}
                                    \tilde{\logLike}(D|\rho_n)\\
            \vect{\nu}_{n+1} & \leftarrow \vect{\nu}_n + \dot{\vect{\nu}}_n.
        \end{aligned}
    \end{equation}

    \noindent
    The stepsize parameter, $\lambda$, limits the rate of change of the distribution moments, and the velocity memory,
    $\eta$ helps the iteration bootstrap itself through local minimums as suggested by \cite{kingma2014adam}. The
    ``velocity'' of each gradient is stored in $\dot{\vect{\mu}}_n$ and $\dot{\vect{\nu}}_n$, respectively.

    \begin{remark}\label{rem:graidient_variability}
	    We do notice that the gradient variability is also a function of the size of $D$, as concluded by
    	\cite{tangkaratt2014model}. Therefore the step size, $\lambda$, is a hyper-parameter that is dependent on the
    	experiment.
    \end{remark}


\subsubsection{Algorithm Termination}\label{sec:policy_infer_terminate}

    In general, the gradient ascent should be terminated at some final iteration $N$ when the update to the parameters
    no longer improves
    $\tilde{\logLike}(D|\rho)$. This log-likelihood is bounded,
    \[
    \log P(D|\paramVec^{(i)}) \leq 0,\  \forall \paramVec \in \varTheta,
    \]
    where $\varTheta$ is the domain of the parameter vector. Due to the nature of sampling, there is no guarantee that
    for every iteration $\logLike(D|\rho_{n+1})> \logLike(D|\rho_n)$. Therefore, we'll use a moving average of the past
    $\Lambda$ log-likelihoods,
    \[
    \mathsf{HIST}(\logLike_n)=\frac{1}{\Lambda}\sum_{v=0}^{\Lambda-1}\logLike(D|\rho_{n-v}).
    \]
    We record the previous value of the moving average, $\mathsf{HIST}(\logLike_{n-1})$, and if the improvement in the
    moving average is below a defined threshold,
    \[
    \Delta\mathsf{HIST}(\logLike_n) = \mathsf{HIST}(\logLike_n) - \mathsf{HIST}(\logLike_{n-1}) \leq \zeta,
    \]
    the algorithm will terminate. We'll require that at least $N$ iterations are performed before termination. Upon
    termination the mean values of $\rho_{n}$ are assigned to the parameter vecture used to build
    $\estimate{\policy{}}_{2}(s;\estimate{\paramVec}),\ \estimate{\paramVec} \leftarrow \vect{\mu}_n.$

    \section{Single-agent policy inference experiment}\label{sec:single_agent_experiment}
    The algorithm and parameterization presented in Sections \ref{sec:policy_parameterization}-\ref{sec:gauss_policy}
    are initially tested in a single agent environment. In this example, the states are just $s=s_2 \in S$, and
    $\policy{1}(a_1|s)$ is dropped from Eq. \ref{eq:true_state_trans_prob}.

    \subsection{Simulation environment}
    Assume that \agent{2} exists alone in a $5\times5$ grid world. In a single agent simulation, states and grid-cells
    are synonymous. The available action set is
    \[
    A = \{Empty, North, South, East, West\},
    \]
    which correspond to their motion primitives
    \[
    \{Stay, Up, Down, Right, Left\}.
    \]
    The motion resulting from an action $a_2$ is stochastic; the intended motion is actually executed with a probability
    of $0.8$, and there is a probability of $0.1$ of sliding in the two perpendicular motion directions. The exception
    is the $Empty$ action which results in a $Stay$ motion with probability $1$. The boarders of the grid-world are
    considered to be walls. If the resulting motion would cause the agent to leave the grid then $Stay$ is selected, $s'=s$.
    Table \ref{table:motion_model} clarifies the motion model for the $East$ action based on the starting cell location.

    \begin{table}[h!]
        \centering
        \begin{tabular}{ c || c | c | c | c}
                & \multicolumn{4}{c}{Starting grid-cell location type}\\
                %\hline
                Resulting Motion & Middle & Right Column & Top Row & Upper Right Corner\\
                \hline
                %      &       &       &       \\
                North & $0.1$ & $0.1$ & $0.0$ & $0.0$ \\
                South & $0.1$ & $0.1$ & $0.1$ & $0.1$ \\
                East  & $0.8$ & $0.0$ & $0.8$ & $0.0$ \\
                West  & $0.0$ & $0.0$ & $0.0$ & $0.0$ \\
                Stay  & $0.0$ & $0.8$ & $0.1$ & $0.9$ \\
        \end{tabular}
        \label{table:motion_model}
        \caption{Agent motion model given attempted \textit{East} (right) action.}
    \end{table}

    The \textit{hidden} policy of \agent{2} is visualized in Fig. \ref{fig:single_agent_2_policy}. The policy is
    deterministic, $\kappa=0$, and the agent has a hidden goal of the yellow cell. The red cells are obstacles, are
    \textit{not} hidden, and are sink states. The \acf{SP} in metric from a cell to a kernel center, Eq. \ref{eq:kernel_func}, accounts for the distance around known obstacles. Arrows represent the direction an action is selected with probability $1$,
    and dots represent that the $Empty/Stay$ action is selected with probability $1$. A demonstration set $D$ is
    generated with a uniform initial distribution $s^{(0)} \sim \mathcal{U},\ \forall \traj_d \in D$. The state
    visitation count for the example $D$ is visualized in Fig. \ref{fig:single_agent_demo}

    \begin{figure}[htb]
        \begin{center}
                \fbox{
                        \begin{minipage}{0.5\textwidth}
                                \includegraphics[width=\textwidth]{single_agent_2_policy}
                                \caption{True Policy of \agent{2}.}
                                \label{fig:single_agent_2_policy}
                        \end{minipage}
                }
        \end{center}
    \end{figure}


    \begin{figure}[htb]
        \begin{center}
                \fbox{
                        \begin{minipage}{0.5\textwidth}
                                \includegraphics[width=\textwidth]{single_agent_demo}
                                \caption{State visitation count in single agent demonstration.}
                                \label{fig:single_agent_demo}
                        \end{minipage}
                }

        \end{center}
    \end{figure}

    \subsection{Experiment Hyper-parameters}
    To approximate the $Q$-function of \agent{2}, per Section \ref{sec:policy_parameterization}, let's start
    by placing a kernel centered at every grid cell. This sets $W=|S|\times A$, but it's a good test. See Fig.
    \ref{fig:kernel_visualization} for a visualization of how the the kernel function values, $k(s,a_2)$, at each state
    are mapped to the feature vector-function $\featFunc$. We'll set the following parameters:

    \begin{table}[H]
        \centering
        \begin{tabular}{c|l l}
                $\kernStdDev_{\kernIdx}$ & $1.1,\ \forall l$ & Identical kernel standard-deviations\\
                $\kappa$ & $0.1$ & Temperature of $\estimate{\policy{}}_2$. Eq. (\ref{eq:policy_model}) \\
                $\lambda$ & $1\mathrm{e}\!-\!5$ & Gradient update rate. Eq. (\ref{eq:gradient_update}) \\
                $\eta$ & $0.0$ & Gradient velocity memory\\
                $m$ & 5000 & Per iteration sample size of $\paramVec\sim \rho$\\
                $\Lambda$ & $60$ & Moving average buffer length for $\mathsf{HIST}(\logLike)$ \\
                $\zeta$ & $0.001$ & Gradient ascent termination when $\Delta\mathsf{HIST}(\logLike) < \zeta$\\
                    $\mu_{0}$ & $0.0$ & Initial parameter means\\
                $\nu_{0}$ & $1.0$ & Initial parameter standard-deviations\\
                $\nu_{min}$ & $0.2$ & Minimum parameter standard-deviation\\
        \end{tabular}
        \caption{Hyper-parameters used for single agent inference.}
        \label{table:single_agent_hyper_params}
    \end{table}

    \begin{table}[H]
	\centering
	\begin{tabular}{c|l l}
		$|\traj|$ & $10$ & Number of steps in a trajectory \\
		$|D|$ & $5000$ & Number of trajectories observed \\
		$I_0$ & $\mathcal{U}(0,24)$ & Uniform distribution of $s_2^{(0)}$ \\
	\end{tabular}
	\caption{Observed data for single agent inference.}
	\label{table:single_agent_data_set}
\end{table}

    \begin{figure}[h]
        \includegraphics[width=\textwidth]{feature_clarification}
        \caption{Feature values for a kernel centered at cell $0$.}
        \label{fig:kernel_visualization}
    \end{figure}

    \subsubsection{Algorithm bounds}

    As noted in footnote 2 of \cite{williams1992simple}, there is not gradient stepsize, $\lambda$ (this reports notation), that will keep the parameter variance elements $\nu_w >0$. Our remedy is to enforce a lower bound on the covariance of $\rho_n$ after each gradient update in Eq. \ref{eq:gradient_update}:
    \begin{equation}\label{eq:param_var_bounds}
    \nu_{min} \leq \vect{\nu}_n,\ \forall \nu_w \in \vect{\nu}_n.
    \end{equation}

	\subsection{Results}\label{sec:single_agent_25K_results}
	Our Assumption
    \ref{assump:opt_policy_err} is verified by Figures \ref{fig:single_agent_logLike_25_kernels} and
    \ref{fig:single_agent_L1Norm_25_kernels}. The recorded $\mathsf{L}_{\infty}$-norm was neither used during gradient ascent,
    nor as a termination criteria. Note that the final $\mathsf{L}_\infty\text{-norm} \approx 0.7$. This final
    error has a range of about $[0.01-5]$ with these parameters over different trials.	We can also examine the dynamics of the distribution $\rho$ as well, see Figures \ref{fig:smooth_gradient_dynamics_mu} and \ref{fig:smooth_gradient_dynamics_nu}.
    
    Note that the legend entry ``$\max_{i \in m}\tilde{\mathcal{L}}_n(D|\mathbf{\theta}^{(i)})$" represents the \emph{sampled} parameter vector that maximizes the log-likelihood of $D$ at the iteration $n$. The policy estimate $\estimate{\pi}_2(\estimate{\paramVec})$ is constructed using $\mu_N$, the mean vect of the multi-variate distribution $\rho$ at the final iteration, $N$.
    
    As the iterations progress, notice that the parameter variances decrease in Fig. \ref{fig:single_agent_7K_nu}, implying that the value of of each parameter is known with more confidence.

    \begin{figure}[H]
        \begin{center}
                \fbox{
                        \begin{minipage}{0.75\textwidth}
                                \includegraphics[width=\textwidth]{logLike_25K_temp_0_1}
                                \caption{$\tilde{\logLike}(D|\rho)$ with parameters in Table \ref{table:single_agent_hyper_params}.}
                                \label{fig:single_agent_logLike_25_kernels}
                        \end{minipage}
                }
        \end{center}
    \end{figure}

    \begin{figure}[H]
        \begin{center}
                \fbox{
                        \begin{minipage}{0.75\textwidth}
                                \includegraphics[width=\textwidth]{L1Norm_25K_temp_0_1}
                                \caption{$\OneNorm{\policy{2},\estimate{\policy{}}_2}$ with parameters in Table
                                        \ref{table:single_agent_hyper_params}.}
                                \label{fig:single_agent_L1Norm_25_kernels}
                        \end{minipage}
                }
        \end{center}
    \end{figure}


 	\begin{figure}[H]
		\begin{center}
			\fbox{
				\begin{minipage}{0.75\textwidth}
					\includegraphics[width=\textwidth]{smooth_grad_descent_mu}
					\caption{$\mu_w$ for each iteration with parameters in Table
						\ref{table:single_agent_hyper_params}.}
					\label{fig:smooth_gradient_dynamics_mu}
				\end{minipage}
			}
		\end{center}
	\end{figure}

 	\begin{figure}[H]
	\begin{center}
		\fbox{
			\begin{minipage}{0.75\textwidth}
				\includegraphics[width=\textwidth]{smooth_grad_descent_nu}
				\caption{$\nu_w$ for each iteration with parameters in Table
					\ref{table:single_agent_hyper_params}.}
				\label{fig:smooth_gradient_dynamics_nu}
			\end{minipage}
		}
	\end{center}
	\end{figure}

    \subsection{Experiment with Fewer Kernels}

	Note that if we use $K=7$, and use the parameters in Table \ref{table:single_agent_new_hyper_params}, we can achieve a final
$\mathsf{L}_{\infty}\text{-norm} \approx 6.5$. There is some obvious generalization error in the inferred policy shown in Fig. \ref{fig:single_agent_policy_7_kernels}. Note that the grid-cells with blue circles in them note only that a kernel is centered at that location, the circle is not proportional to $\kernStdDev_{\kernIdx}$. Also, the magnitudes of the circles and arrows are proportional to the probability of selecting that action; the yellow cell is in the most North-East cell.

\todo[inline]{Format this table similar to Table \ref{table:single_agent_hyper_params}.}
    \begin{table}[H]
        \centering
        \begin{tabular}{c|c}
                $\kernStdDev_{\kernIdx}$ & $\mathbf{2.0},\ \forall l$ -- Identical kernel standard-deviations.\\
                $\kappa$ & $0.1$ -- Temperature of $\estimate{\policy{}}_2$. Eq. (\ref{eq:policy_model}). \\
                $\lambda$ & $\mathbf{1\mathrm{e}\!-\!6}$ -- Gradient update rate. Eq. (\ref{eq:gradient_update}) \\
                $\eta$ & $\mathbf{0.2}$ -- Gradient velocity memory.\\
                $m$ & 5000 -- Per iteration sample size of $\paramVec\sim \rho$.\\
                $\Lambda$ & $60$ - Moving average buffer length for $\mathsf{HIST}(\logLike)$. \\
                $\zeta$ & $0.001$ Gradient ascent termination when $\Delta\mathsf{HIST}(\logLike) < \zeta$.\\
                $\nu_{min}$ & $0.2$ -- Minimum parameter standard-deviation.\\
        \end{tabular}
        \caption{Hyper-parameters for inference with $K=7$.}
        \label{table:single_agent_new_hyper_params}
    \end{table}

    \begin{figure}[htb]
	\begin{center}
		\fbox{
			\begin{minipage}{0.75\textwidth}
				\includegraphics[width=\textwidth]{single_agent_learned_7Kernels}
				\caption{$\estimate{\policy{}}_{2}$ with parameters in Table \ref{table:single_agent_new_hyper_params}.}
				\label{fig:single_agent_policy_7_kernels}
			\end{minipage}
		}
	\end{center}
	\end{figure}

    \begin{figure}[htb]
        \begin{center}
                \fbox{
                        \begin{minipage}{0.75\textwidth}
                                \includegraphics[width=\textwidth]{Loglike_single_agent_7K}
                                \caption{$\tilde{\logLike}(D|\rho)$ with parameters in Table \ref{table:single_agent_new_hyper_params}.}
                                \label{fig:single_agent_logLike_7_kernels}
                        \end{minipage}
                }
        \end{center}
    \end{figure}


    \begin{figure}[H]
	\begin{center}
		\fbox{
			\begin{minipage}{0.75\textwidth}
				\includegraphics[width=\textwidth]{Linf_norm_single_agent_7K}
				\caption{$\OneNorm{\policy{2},\estimate{\policy{2}}}$ with parameters in Table
					\ref{table:single_agent_new_hyper_params}.}
				\label{fig:single_agent_L1Norm_7_kernels}
			\end{minipage}
		}
	\end{center}
\end{figure}

    \begin{figure}[htb]
        \begin{center}
                \fbox{
                        \begin{minipage}{0.75\textwidth}
                                \includegraphics[width=\textwidth]{grad_descent_mu_7K}
                                \caption{$\mu_w$ with parameters in Table
                                        \ref{table:single_agent_new_hyper_params}.}
                                \label{fig:single_agent_7K_mu}
                        \end{minipage}
                }
        \end{center}
    \end{figure}


    \begin{figure}[htb]
	\begin{center}
		\fbox{
			\begin{minipage}{0.75\textwidth}
				\includegraphics[width=\textwidth]{grad_descent_nu_7K}
				\caption{$\nu_w$ with parameters in Table
					\ref{table:single_agent_new_hyper_params}.}
				\label{fig:single_agent_7K_nu}
			\end{minipage}
		}
	\end{center}
\end{figure}




