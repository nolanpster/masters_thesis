%
% Since this is a ``report'', the topmost level of hierarchy is
% ``Chapter'', not section as you may be used to. Chapters are
% enumerated starting from 1, so Sections are 1.1, Subsections are
% 1.1.1, subsubsections don't get numbers. (You can change that, if
% you want them to be called 1.1.1.1)
%
\chapter{Policy Inference with Gaussian Policy Estimate}\label{chapt:gauss_policy}

    The multi-agent system presented in this thesis is modeled with two agents, the controllable agent \agent{1} and the
    uncontrollable agent \agent{2}. The interraction between the two agents is captured by a \acf{MDP}. We assume that
    \agent{2} has a predefined yet unknown policy. If \agent{1} is given a task, it can only robustly plan for this task
    when it has an accurate estimate of how \agent{2} will act. Determining the action distribution of an uncontrollable
    agent given its current state is known as \textit{policy-inference}. The following definitions formally present a
    hidden policy in \iac{MDP}.

    After defining the hidden-parameter MDP, this chapter presents an inference method that uses Monte-Carlo integration
    of policy parameters sampled from a multivariate distribution. We'll use \ac{SGA} to improve the log-likelihood of
    an observed set of data given a policy parameterization. Section \ref{sec:policy_parameterization} details a
    $Q$-function approximation which is used to form a softmax policy like in \cite{nachum2017bridging}, also known as a
    Boltzman \cite{Hanawal2017LearningPolicies} and a Gibbs \cite{Sugiyama2015StatisticalRL} policy.

\section{Hidden-parameter MDP}\label{sec:hipmdp}
    \begin{definition}\label{def:hipmdp}
        The interaction between two agents is captured by a hidden-parameter \ac{MDP},
        \[
            \mathcal{M} = (S, A_1 \times A_2, R, T, \policy{2}, I_0 , \gamma)
        \]
        with the tuple defined as in \cite{Sugiyama2015StatisticalRL} plus the hidden parameter, $\policy{2}$:
        \begin{itemize}
            \item $S \equiv (S_1 \times S_2)$ is a set of joint states with cardinality $0 < |S| < \infty$ .
            \item $A_1\times A_2$ is a finite set of actions, where $A_1$ is the set agent one can execute and $A_2$ is
                the set available to agent two.
            \item $R: S\times A_1 \rightarrow \reals$ is the real-valued state-action reward function given to the
                controllable agent.
            \item $T: S\times (A_1\times A_2)\rightarrow \dist(S)$ is the probabilistic state transition function
                $ T(s'| s, (a_1,a_2)) $ which yields the probability of reaching state $\textnormal{s}'$ after both
                agents take action pair $(\text{a}_1,\text{a}_2)$ at the state $\textnormal{s}$.
            \item $\policy{2} : S \rightarrow \dist(A_2)$ The distribution of agent two's actions given a
                state. The probability of each action is $\policy{2}(a_2|s)$.
            \item $I_0 \in \dist(S)$ is the initial state distribution.
            \item $\gamma \in (0,1]$: The discounting factor.
        \end{itemize}
    \end{definition}

    \noindent
    This definition also leads us to a couple pivotal assumptions in this work:

    \begin{assumption}
        The state-action transition function $T(\cdot)$ is known.
    \end{assumption}

    We'll also need two definitions from \cite{Guo2009} about policies to continue:

    \begin{definition}{Markov Policy:} \label{def:markov_policy}
        A policy $\pi_t(s)$ is considered to be a Markov policy if $P(a_j|h_t) = P(a_j|s^{(N)}), \forall a_j \in A$,
        where $h_t = s^{(0)},...,s^{(N)}$ is a state history ending at some discrete time $t \in [0,\infty)$.
    \end{definition}

    \begin{definition}{Stationary Policy:} \label{def:stationary_policy}
        A policy $\pi_t(s)$ is considered to be a stationary policy if $\pi_t(s_i) = \pi(s_i), \forall s_i \in S$,
        for all time $t \in [0,\infty)$.
    \end{definition}

    Thus, a stationary Markov policy is fixed with respect to time, and is only dependent on the current state of
    the system; it is independent of history.

    \begin{assumption}\label{assump:stationary_markov}
        The unknown policy of \agent{2}, $\policy{2}$ is a stationary Markov policy.
    \end{assumption}

    Given that \agent{1} takes some action $a_1$, the distribution of next states is
    \begin{equation}\label{eq:true_state_action_trans_prob}
        P(s'| s, a_1) = \sum_{a_2\in A_2} T\left(s'|s, (a_1,a_2)\right)\policy{2}(a_2|s).
    \end{equation}

    \noindent
    With this model, the true probability of a future state $s'$ given the current state $s$ is
    \begin{equation}\label{eq:true_state_trans_prob}
        p(s'|s) = \sum_{a_1, a_2 \in A} T \left(s'|s, (a_1,a_2)\right)\policy{2}(a_2|s)\policy{1}(a_1|s),
    \end{equation}

    \noindent
    where, for simplicity, this report uses identical action sets, $A_1 \equiv A_2 \equiv A$. Finally, the \ac{MDP}
    formed by the tuple $(S, A_1 \times A_2, R, T, \policy{2}, I , \gamma)$ will be referred to as $\mathcal{M}$.

\section{Unknown Policy Parameterization}

\subsection{Preliminaries}\label{sec:policy_iteration_preliminaries}
    The parameterization for \policy{2} requires a an understanding of how a reward function, $R$, influences the
    optimal action distribution at a state $s$. Although we do not wish to learn the reward function of \agent{2}, we
    will build approximations of the \textit{value} and \textit{state-action value} functions used by \agent{2}. These
    are often a function of a reward function.

\subsubsection{Value Functions}
    Following \cite{hernandez2012adaptive} and \cite{Sugiyama2015StatisticalRL}, the value of a state $s$ is defined as the
    expectation over all discounted future rewards that could be earned from $s$. For an arbitrary, single-agent \ac{MDP},
    if an agent \agent{i} follows a policy \policy{i}:
    \[
    V_{\policy{i}}(s) = \expectation{\policy{i}}{\sum_{t=0}^{\infty} \gamma^t R\left((s,a_i)^{(t)}\right)
            \bigg|s^{(t)}=s}.
    \]
    The subscript in $\expectation{\policy{i}}{\cdot}$ means that the expectation is taken over the single agent Markov
    chain that is induced by \agent{i} playing policy $\policy{i}$ in the \ac{MDP},
    \[
    V_{\policy{i}} = \expectation{\policy{i}}{R\left((s,a_i)^{(t)}\right) + \gamma V_{\policy{i}}(s^{(t+1)})}.
    \]

\subsubsection{State-action Value (Q) Functions}
    Likewise, the value of an action at a particular state is the expected discounted future reward following policy
    \policy{i}:
    \[
    Q_{\policy{i}}(s,a_i) = \expectation{\policy{i}}{\sum_{t=0}^{\infty} \gamma^t R\left((s,a_i)^{(t)}\right)
            \bigg|s^{(t)}=s, a_i^{(t)}=a_i}.
    \]


\subsection{Q-function Approximation}\label{sec:policy_parameterization}
    The model of $\policy{2}(s; \paramVec)$ approximates the \textit{state-action value function}, $Q(s,a_2)$, with a
    linear-in-parameter model. Sugiyama describes this model in detail in Section 2.2.1 of
    \cite{Sugiyama2015StatisticalRL}, it is also used by \cite{Hanawal2017LearningPolicies}. It is advantageous that
    \agent{1} does not need to learn the reward function of \agent{2}, but only the distribution of \agent{2}'s action
    given the joint state set, $\policy{2}(s),\ s\in S$. Each parameter element determines the weighting of a feature
    $\phi : S \times A_2 \rightarrow \reals$. If a total of $\paramLen$ features are used then \agent{2}'s
    \textit{Q}-function is approximated as
    \begin{equation*}\label{eq:QFuncApprox}
    Q(s,a_2) \approx \estimate{Q}(s,a_2)
    = \sum_{\paramIdx=1}^{\paramLen} \estimate{\paramSym}_{\paramIdx} \featElem(s,a_2)
    = \estimate{\paramVec}^\top \featFunc.
    \end{equation*}

    Following Chapter 3 of \cite{Sugiyama2015StatisticalRL}, we'll use \acp{GGK} because they intuitively incorporate
    any obstacles\footnotemark\ that exist in $S$ into a distance metric between a state $s$, and the center of a
    \ac{GGK}, $c$. Consider the state-graph of $\mathcal{M}$. If there are $K$ kernels, and the $l$-th kernel has center
    $c$, then the value of the kernel at a state $s$ is
    \begin{equation*}
        \kernFunc(s,\kernCent_{\kernIdx}) = \exp \left( \frac{-\text{SP}(s,\kernCent_{\kernIdx})^2}
                                                             {2\kernStdDev^2_{\kernIdx}} \right).
    \end{equation*}

    \footnotetext{Obstacles could be considered as known sink-states in $\mathcal{M}$, and will be clarified in Sect.
                  \ref{sec:single_agent_experiment}.}

    \noindent
    The \ac{SP} between a state $s$ and the $l$-th kernel's center $c_l$ can be precomputed, and the kernel standard
    deviation, $\kernStdDev_{\kernIdx}$, determines the effective support of the kernel. The feature function is defined
    for each action, so the number of features is $W = |A| \times K$. The $w$-th feature function is
    \begin{equation}\label{eq:kernel_func}
        \featElem(s_2,a_2) = I(a_2==a_2^{(j)})\sum_{s_2'\in S_2}P\left(s_2'|s_2,a_2^{(j)}\right)
                                \kernFunc(s_2',c_{\kernIdx}),
    \end{equation}

    \noindent
    where $w=j+(\kernIdx|A_2|-1)$ represents the index used for both feature-vector functions and parameter elements,
    \paramElem. Also, for each action $a_2$ the indicator function ${I(a_2==a_2^{(j)})}$ is defined as:
    \begin{equation*}
        \begin{aligned}
            I(a_2==a_2^{(j)}) & = \begin{cases}
                                    0 & \text{if}\ a_2 \neq a_2^{(j)} \\
                                    1 & \text{if}\ a_2 = a_2^{(j)}
                                  \end{cases} \\
            j & = 1, \ldots, |A_2|,
        \end{aligned}
    \end{equation*}

    \noindent
    assuming all actions are enabled from each state $s_2 \in S_2$.

    Given a $Q$-function, \cite{nachum2017bridging} represents the policy at a state as represented the softmax over all
    actions,
    \begin{equation}\label{eq:policy_model}
        \policy{2}(a_2|s) = \exp((Q(s,a_2)- V(s))/\kappa),
    \end{equation}

    \noindent
    where $V(s) = \kappa\log \sum_{a_2\in A}\exp (Q(s,a)/\kappa)$. Also, $\kappa$ is a temperature parameter. If $\kappa
    \rightarrow \infty$, the distribution $ \policy{2}(s) $ becomes uniform over $A_2$. If $\kappa \rightarrow 0$, the
    distribution $\policy{2}(s)$ becomes a Dirac Delta distribution that peaks on the action $a_2^\ast = \argmax_{a_2}
    Q(s,a_2)$. In the inference algorithm, we consider $\kappa$ as a predefined hyper-parameter in the range
    $(0,\infty)$.

    This is the model used for \policy{2} in the rest of this report. The advantage of this parameterization is that we
    have reduced the number of parameters to learn from $|S|\times A$ to $W$. We'll show that we can effectively infer a
    policy with $K \ll |S|$.

    The form used in Eq. \ref{eq:policy_model} is equivalent to the Gibbs policy from \cite{Sugiyama2015StatisticalRL}.
    It is referred to as a Boltzmann policy in \cite{Hanawal2017LearningPolicies}, and \cite{herman2016inverse},
    although the authors have described this parameterizion without reference to a temperature parameter, in which case
    $\kappa=1$.

\section{Policy Inference}\label{sec:policy_obj}

    For \agent{1} to plan a near-optimal policy for its own task, it must learn the policy of the other agent, which
    gives the model of the MDP. The estimate of \agent{2}'s policy, $\estimate{\policy{}}_2$, is parameterized by a
    vector \vect{\theta}.  Therefore, the estimated probability of a state transition is
    \begin{equation}\label{eq:est_state_trans_prob}
        q(s'|s, \vect{\theta}) = \
            \sum_{a_1, a2 \in A}P(s'|s,(a_1,a_2))\estimate{\policy{}}_2(a_2|s,\vect{\theta})\policy{1}(a_1|s).
    \end{equation}

    As the \agent{2} moves through $S$, the robot agent can observe the outcomes of \agent{2}'s actions, and build a set
    of observed state sequences.
    \begin{definition}\label{def:traj}
        A trajectory $\tau$ is a sequence of joint states $s=(s_1, s_2)$ with time-step index $t$,
        \[
        s^{(0)}, s^{(1)}, \ldots , s^{(t)}, \ldots , s^{(|\tau|)};\ 0 \leq t \leq |\tau|.
        \]
    \end{definition}

    \begin{remark}
        This definition is a key difference from comparable policy gradient algorithms, such as \ac{pgpe}
        \cite{tangkaratt2014model} \cite{sehnke2010parameter}. In lieu of state-action sequences, Section
        \ref{sec:gauss_policy} will present an inference procedure that uses observed state-\textnormal{action-outcome}
        sequences. The action-outcome is assigned to be the most probable \textnormal{(maximum a posteriori)} action
        that can lead from $\textnormal{s}$ to $\textnormal{s'}$ using the known transition function, $T(\cdot)$.
    \end{remark}

    Suppose the policy of agent one, \policy{1} is known. We'll define the probability of a trajectory as the joint
    probability of each set state-transition tuple for each of the two distributions, $p$ and $q$:
    \begin{align*}
        p(\tau) &= \prod_{t=1}^{|\tau|}p\left( s^{(t)}| s^{(t-1)} \right), \\
        q(\tau|\vect{\theta}) &= \prod_{t=1}^{|\tau|}q\left(s^{(t)}| s^{(t-1)}, \vect{\theta}\right).
    \end{align*}

    \noindent
    By using the parameter \paramVec, $q(\tau,\paramVec)$ is the probability of replicating a trajectory given the
    parameterization.

    \begin{assumption}
        The observed demonstration set, D, is sampled i.i.d. from the set of all possible demonstrations $\mathcal{D}$.
    \end{assumption}

    The best inference of an environmental policy has a high likelihood of replicating the trajectories in $D$.
    \begin{lemma}\label{lemma:obj_fun_equiv}
        Minimizing the \ac{KLD} of the replica distribution from the observed trajectory distribution is equivalent to
        maximizing the log-likelihood of the observed state sequences given a parameterized policy. With a fixed policy
        for \agent{1}, $\policy{1}$,
        \begin{equation*}
            \argmax_{\paramVec} \logLike(\paramVec) = \argmin_{\paramVec} \text{KL}(p||q_{\paramVec}).
        \end{equation*}
    \end{lemma}

    \begin{proof}
        Consider a pair of stationary policies for the two agents, $\policy{1}$ and $\policy{2}$. The induced Markov
        chain is $\mathcal{M}_{\policy{1}, \policy{2}}$. Let $p$ be the probability distribution of paths in the chain
        $\mathcal{M}_{\policy{1}, \policy{2}}$. Let $q_{\paramVec}$ be the probability distribution of paths in the
        chain $M_{\policy{1}, \estimate{\policy{}}_2}$. The \ac{KLD} from $q_{\paramVec}$ to $p$ is
        \begin{equation*}\label{eq:traj_kl_div}
            \text{KL}(p || q_{\paramVec}) = \sum_{\traj_d \in D} p(\traj_d) \ln \left( \frac{p(\traj_d)}
                                                {q(\traj_d|\paramVec)} \right)\
                                          = \sum_{\traj_d \in \calD} P(\traj_d|D) \ln \left( \frac{P(\traj_d|\calD)}
                                                {P(\traj_d|\paramVec)} \right),
        \end{equation*}

        \noindent
        where $P(\traj_d|D)$ is the maximum likelihood probability of the state sequence, and $P(\traj_d|\paramVec)$ is
        the probability of obtaining that state sequence by our inferred policy that is parameterized by the vector
        \paramVec.

        Minimizing the deviation of $q_{\paramVec}$ from $p$ is equivalent to maximizing the expectation of the
        observing $D$, given that the environment actions are distributed as $\policy{2}(s; \paramVec)$:
        \begin{equation}\label{eq:min_kld}
            \begin{aligned}
                \argmin_{\paramVec}(\text{KL}(p || q_{\paramVec})) & = \
                    \argmin_{\paramVec}\left(\sum_{\traj_d \in  \calD}\!  P(\traj_d|\calD)
                    \ln\left(\frac{P(\traj_d|\calD)}{P(\traj_d|\policy{1},\paramVec)}\right)\right)\\
                & = \argmin_{\paramVec}\bigg(\underbrace{\sum_{\traj_d \in D} P(\traj_d|D) \ln(P(\traj_d|D))}_{constant} -
                    P(\traj_d|D) \ln\left(P(\traj_d|\policy{1},\paramVec)\right)\bigg)\\
                & = \argmax_{\paramVec}\left(\sum_{\traj_d \in \calD}\!  P(\traj_d|\calD)
                    \ln\left(P(\traj_d|\policy{1},\paramVec)\right)\right)\\
                & = \argmax_{\paramVec}\expectation{P(\traj_d|\calD)} {\ln(P(\traj_d|\policy{1},\paramVec))} \\
                &\approx \argmax_{\paramVec} \sum_{\traj_d \in D}  \ln(P(\traj_d|\policy{1},\paramVec))\\
                & =\argmax_{\paramVec} \logLike(D|\policy{1},\paramVec)
            \end{aligned}
        \end{equation}

        \noindent
        where we estimate the expectation using the empirical mean. We will write the final line of Eq.
        \ref{eq:min_kld} as $\logLike(\paramVec; \policy{1})$ for compactness and consistency with Lemma
        \ref{lemma:obj_fun_equiv}.

    \end{proof}

    For the rest of this report, lets assert that an optimal parameter exists.
    \begin{assumption}\label{assump:opt_policy_err}
        There exists an optimal parameter vector that can represent the true distribution of $\policy{2}(s)$ to within a
        threshold $\xi$, given that a set of basis functions are properly defined;
        \[
            \exists\ \optimal{\paramVec}\ \Big|\  \InfNorm{\estimate{\policy{}}_2(s;\optimal{\paramVec}),\policy{2}(s)}
                \leq \xi.
        \]
    \end{assumption}

    \noindent
    The infinite $\mathsf{L_{\infty}}$-norm between two policies,
    \[
        \InfNorm{\policy{x}(s),\policy{y}(s)} = \sum_{s \in S}\sum_{a \in A}\policy{x}(a|s)-\policy{y}(a|s),
    \]
    is a measurable distance unlike \ac{KLD}; $\text{KL}(p||q) \neq \text{KL}(q||p)$. For all following experiments,
    we'll use the $\mathsf{L_{\infty}}$-norm to compare two policies.

    We are now ready to discuss the inference procedure used to identify the best estimate of $\policy{2}(s, \paramVec)$
    that maximizes the R.H.S of Lemma \ref{lemma:obj_fun_equiv}.


\subsection{Gaussian Distribution of Policy Parameters}\label{sec:gauss_policy}

    After a data set of trajectories, $D$, has been collected, \agent{1} needs to maximize $\logLike(D|\paramVec)$, the
    log-likelihood of the dataset when \policy{2} is parameterized by an estimated parameter vector
    \estimate{\paramVec}. Let $\estimate{\paramVec} = [ \paramElem]_{w=1}^W$ be a vector of independently sampled random
    variables with Gaussian distributions $\mathcal{N}(\mu_w, \nu_w^2)$ for $w=1,\ldots,W$. The variance, $\nu_w^2$,
    will capture the uncertainty of $\paramElem$ in the inference from dataset $D$.

    The following is similar to the analysis in \cite{tangkaratt2014model}, \cite{herman2016inverse}, and
    \cite{sehnke2010parameter} except that we do not include a reward function because the policy sought must replicate
    the observed data $D$, not earn a reward.

    We denote $\rho_w = (\mu_w, \nu_w)$ as the tuple of mean and variance for $\paramElem$ and denote $\rho
    =\{\rho_w\}_{w=1}^W$ to be the collection of variable tuples. Given $\rho$, the probability of the demonstrations is
    \[
        P(D |\rho) = \int_{\paramVec } P(D|\paramVec) p(\paramVec | \rho)d\paramVec.
    \]

    \noindent
     The log-likelihood of the demonstrations can be lower-bounded using Jensen's inequality:
    \begin{equation*}
    \logLike(D|\rho ) = \log P(D|\rho)\
         = \log \left( \int_{\paramVec}P(D|\paramVec) p(\paramVec | \rho)d\paramVec \right)\ \ge \int_{\paramVec}
            p(\paramVec|\rho)\log \big( P(D|\paramVec) \big) d\paramVec.
    \end{equation*}

    \par
    Denote this lower bound as $\tilde{\logLike}(D|\rho)=\int_{\paramVec} p(\paramVec |\rho) \log P(D|\paramVec)
    d\paramVec$. This is the lower bound on the objective function derived in Eq. \ref{eq:min_kld}. By taking
    derivative of $\tilde{\logLike}(D|\rho)$ with respect to $\rho$, we obtain the gradient of the objective function:
    \begin{equation}\label{eq:gradLogLike}
        \begin{aligned}
            \nabla_\rho \tilde{\logLike}(D|\rho)) & =
                \int_{\paramVec}\nabla_\rho p(\paramVec|\rho) \log P(D|\paramVec)d\paramVec\\
            & = \int_{\paramVec}[ p(\paramVec|\rho) \nabla_\rho \log p(\paramVec|\rho) ] \log P(D|\paramVec)d\paramVec\\
            &\approx \frac{1}{m} \sum_{ i=1}^m \left[\nabla_\rho \log P(\paramVec^{(i)}|\rho) \right] \log
                P(D|\paramVec^{(i)})
        \end{aligned}
    \end{equation}

    \noindent
    where $\paramVec^{(i)}$, $i=1,\ldots, m$ are samples generated from the multi-variant Gaussian distribution with
    mean $\vect{\mu} = [\mu_1,\ldots, \mu_W]^\top$ and covariance matrix $\mbox{diag}\left(\nu_1,\ldots, \nu_W\right)$.
    Let $\vect{\nu}$ be an equivalent representation for $\mbox{diag}\left(\nu_1,\ldots, \nu_W\right)$. Each sampled
    parameter element, $\paramElem^{(i)}$, has probability:
    \[
        P(\paramElem^{(i)} | \rho_{\paramIdx}) = \frac{1}{\sqrt[]{2\pi \sigma_w^2}}\exp
            \left( -\frac{(\theta_w^{(i)}-\mu_w)^2}{2\sigma_w^2} \right).
    \]

    \par
    The bracketed gradient in the last line of Eq. \ref{eq:gradLogLike} with respect to each element of \vect{\mu} and
    $\vect{\nu}$ are:
    \[
    \nabla_{\mu_{\paramIdx}}\log P(\paramElem^{(i)} | \rho_{\paramIdx}) =
        \frac{\paramElem^{(i)} - \mu_{\paramIdx}}{\nu_{\paramIdx}^2},\ \text{and}
    \]
    \[
    \nabla_{\nu_{\paramIdx}}\log P(\paramElem^{(i)} | \rho_{\paramIdx}) = \frac{(\paramElem^{(i)} - \mu_{\paramIdx})^2 -
        \nu_{\paramIdx}^2}{\nu_{\paramIdx}^3}.
    \]
    Note that superscripts enclosed in parenthesis represent sample indexes, e.g., the $i$-th sample of parameter
    element $w$ is $\paramElem^{(i)}$. All purely numeric superscripts are exponents.

    We can obtain the optimal collection of parameters $\optimal{\rho}= \argmax_{\rho} \tilde{\logLike{}}(D|\rho)$ by
    performing gradient ascent on the parameter distributions, $\rho = (\vect{\mu}, \vect{\nu})$. The policy
    parameterized by $\optimal{\estimate{\paramVec}} \sim \mathcal{N}(\optimal{\vect{\mu}}, \optimal{\vect{\nu}})$ is
    $\estimate{\policy{}}_{2}(s; \optimal{\estimate{\paramVec}})$ and it maximizes the log likelihood of the
    demonstration set $D$. The log likelihood of observed demonstrations for a given $\paramVec^{(i)}$ can be computed
    as
    \begin{align*}
        \log P(D|\paramVec^{(i)}) & = \sum_{\tau_d \in D} \log P\left(\tau_d |\paramVec^{(i)}\right)\\
        & = \sum_{d=1}^{\abs{D}} \left[ \sum_{t=0}^{\abs{\tau_d}-1} \log P \left(s^{(t+1)}|(s, a_1, o_2)^{(t)}\right) +
            \sum_{t=0}^{\abs{\tau_d}-1} \log \policy{2}\left(o_2^{(t)}|s^{(t)}; \paramVec^{(i)}\right) \right]\\
        & = \sum_{s\in S}\sum_{o_2\in A} C(s, o_2) \log \policy{2}\left(o_2|s; \paramVec^{(i)}\right) + Const.
    \end{align*}

    Above, $C(s, o_2)$ is the number of times the state action pair $(s,o_2)$ is observed from in $D$. The constant
    term, $Const =\sum_{d=1}^{\abs{D}} \sum_{t=0}^{\abs{\tau_d}-1} \log P\left(s^{t+1}|(s ,a_1,o_2)^{(t)}\right)$, is
    independent of $\paramVec^{(i)}$ and can be precomputed for a demonstration $D$. The observed action outcome at
    time-step $t$ is $o_2^t$, which is the only action information available in a trajectory, per Definition
    \ref{def:traj}. If a trajectory fragment $(s_1, s_2)^{(t)}, (s_1', s_2')^{(t+1)}$ is observed, the action of the
    controllable agent, $a_1^{(t)}$, is known but the uncontrolled action, $a_2^{(t)}$, is not. Therefore $o_2^{(t)}$ is
    assigned to be the nominal motion that causes the transition $s_2^{(t)} \rightarrow s_2^{(t+1)}$ in the graph of
    $\mathcal{M}$.

    \begin{remark}
        If the policy does not depend on a basis function $\phi_w$, $\mu_w=0$ and $\nu_w \ll 1$, the basis function can
        be removed from the parameterization since it has no influence on the inferred policy.
    \end{remark}

\subsubsection{Gradient Ascent}

    Using the gradient defined in Eq. \ref{eq:gradLogLike}, for each iteration $n$ we sample a set of $m$ parameter
    vectors, and update the distribution parameters on each iteration:
    \begin{equation}\label{eq:gradient_update}
        \begin{aligned}
            \dot{\vect{\mu}}_n & \leftarrow \eta\dot{\vect{\mu}}_{n-1} \lambda\nabla_{\vect{\mu}_n}
                                    \tilde{\logLike}(D|\rho_n)\\
            \vect{\mu}_{n+1} & \leftarrow\ \vect{\mu}_n + \dot{\vect{\mu}}_n\ \text{and}\\
            \dot{\vect{\nu}}_n & \leftarrow \eta\dot{\vect{\nu}}_{n-1} \lambda\nabla_{\vect{\nu}_n}
                                    \tilde{\logLike}(D|\rho_n)\\
            \vect{\nu}_{n+1} & \leftarrow \vect{\nu}_n + \dot{\vect{\nu}}_n.
        \end{aligned}
    \end{equation}

    \noindent
    The stepsize parameter, $\lambda$, limits the rate of change of the distribution moments, and the velocity memory,
    $\eta$ helps the iteration bootstrap itself through local minimums as suggested by \cite{kingma2014adam}. The
    ``velocity'' of each gradient is stored in $\dot{\vect{\mu}}_n$ and $\dot{\vect{\nu}}_n$, respectively.

    We do notice that the gradient variability is also a function of the size of $D$, as concluded by
    \cite{tangkaratt2014model}. Therefore the step size, $\lambda$, is a hyper-parameter that is dependent on the
    experiment.

\subsubsection{Algorithm Termination}\label{sec:policy_infer_terminate}

    In general, the gradient ascent should be terminated at some final iteration $N$ when the update to the parameters
    no longer improves
    $\tilde{\logLike}(D|\rho)$. This log-likelihood is bounded,
    \[
    \log P(D|\paramVec^{(i)}) \leq 0,\  \forall \paramVec \in \varTheta,
    \]
    where $\varTheta$ is the domain of the parameter vector. Due to the nature of sampling, there is no guarantee that
    for every iteration $\logLike(D|\rho_{n+1})> \logLike(D|\rho_n)$. Therefore, we'll use a moving average of the past
    $\Lambda$ log-likelihoods,
    \[
    \mathsf{HIST}(\logLike_n)=\frac{1}{\Lambda}\sum_{v=0}^{\Lambda-1}\logLike(D|\rho_{n-v}).
    \]
    We record the previous value of the moving average, $\mathsf{HIST}(\logLike_{n-1})$, and if the improvement in the
    moving average is below a defined threshold,
    \[
    \Delta\mathsf{HIST}(\logLike_n) = \mathsf{HIST}(\logLike_n) - \mathsf{HIST}(\logLike_{n-1}) \leq \zeta,
    \]
    the algorithm will terminate. We'll require that at least $N$ iterations are performed before termination. Upon
    termination the mean values of $\rho_{n}$ are assigned to the parameter vecture used to build
    $\estimate{\policy{}}_{2}(s;\estimate{\paramVec}),\ \estimate{\paramVec} \leftarrow \vect{\mu}_n.$


\section{Proactive Inference}

    Assuming that \agent{1} has received some initial data $D^{(1)}$, the estimated policy of the uncontrollable agent
can be improved from some initial guess $\estimate{\policy{}}_2^{(0)}$ to form $\estimate{\policy{}}_2^{(1)}$. Let's
assume that $D^{(1)}$ was incomplete, it did not contain enough data for the following algorithm to meet the
tolerance required by Assumption \ref{assump:opt_policy_err}. Therefore, \agent{1} will need to gather more data,
trajectories, $D^{(b)},\ b=1,\ldots,B$. Ideally, $D^{(b)}$ will contain data that were lacking in all previous
batches. When there are not enough data, some estimates parameter elements, $\estimate{\theta}_\paramIdx$, will be
incorrect.

How can \agent{1} glean more informative data? To achieve proactive policy inference, \agent{1} needs to recognize what
parts of \policy{2} it thinks are known and unknown. Explicitly, after observing $D^{(b)}$, \agent{1} still might have
very little knowledge about $\policy{2}(x)$ -- perhaps $s_2=x$ has not yet been visited. This section will discuss how
to distinguish known and unknown policy parameters, and how \agent{1} can proactively influence the observed data, $D$.

\begin{remark}
        The parameter vector inferred from any batch will be written $\estimate{\paramVec}^{(b)}$, which is not
        to be confused with the sampled $\paramVec^{(i)}$ in Eq. \ref{eq:gradLogLike}.
\end{remark}

\subsection{Characterizing Unknown Parameters}\label{sec:unknown_params}
    By Eq. \ref{eq:true_state_trans_prob}, the transition function is determined by the policies of both agents,
    \[
    P((s_1',s_2')|(s_1,s_2),(a_1,a_2)) = T(s_1'|s_1,a_1) \policy{2}(a_2|(s_1,s_2))\policy{1}(a_1|(s_1,s_2)).
    \]
    Since we've made Assumption \ref{assump:stationary_markov} that \policy{2} is a stationary Markov policy, learning
    $\policy{2}$ is equivalent to learning the dynamics of the MDP. Both problems have the same sample complexity, they
    are polynomial in the size of the joint state space and the action space of the \agent{2}. With the Gaussian policy
    approximation from Section \ref{sec:gauss_policy}, if the number of parameters, $W$ is much less than $|S|\times A$,
    then sample complexity will be significantly reduced.

    \begin{definition}
        Given two parameters $\epsilon, \varphi \in (0,1)$, a parameter $\theta_{\paramIdx}$ is $(\epsilon,
        \varphi)$-known if, with probability $1-\varphi$, the probability of the true value for $\paramElem$ is
        $\epsilon$ close to the mean of the Gaussian distribution $\normal{\mu_{\paramIdx}, \nu_{\paramIdx}}$, \ie,
        \[
            P(\abs{\paramElem - \mu_{\paramIdx}} \ge \epsilon) \le \varphi.
        \]
    \end{definition}

    If the estimated unknown policy parameter $\estimate{\paramSym}_w$ is $\epsilon$-close to the true (unknown) mean
    $\mu_{\paramIdx}$ with probability $1-\varphi$, then we could claim  that enough knowledge has been obtained for
    $\paramElem$ and treat it as a \emph{known} policy parameter. This claim is based on the Chernoff bound
    \cite{kobayashi2011probability}.

    Let $\Theta_\known$ be the set of known policy parameters and $\Theta_{\unknown}$ be the remaining unknown
    parameters.  Next, we present a method to determine whether a state is unknown from the knowledge of these sub-sets
    of $\Theta$.

    First, given that each sampled parameter vector in Eq. \ref{eq:gradLogLike}, $\paramVec^{(i)}$, is a Gaussian
    variable, the $\estimate{Q}$-function is
    \[
        \estimate{Q}(s,a ;\paramVec^{(i)}) \sim \sum_{w=1}^W \featElem(s,a_2) \normal{\mu_w, \nu_w^2},
    \]
    The features, $\featElem(s,a_2),\ w=1,\ldots, W$ can be viewed as the mixing parameters.

    A state $s$ is known if and only if for any $a_2 \in A$, $Q(s,a_2)$ is known. Since the $Q$-value is a mixture of
    Gaussians, we'll use the mean and variance of $Q(s,a;\estimate{\paramVec}^{(b)})$ returned from Eq.
    \ref{eq:gradient_update} to determine the distribution of a random variable -- the estimate of $\policy{2}(a|s)$.

    If we were trying to solve a Probably-Approximatly-Correct- (PAC)-MDP \cite{Fu-RSS-14}, we would require an accuracy
    in learning the transition function. For example, if the model error $\bar T(s'|s,a_1) - T(s'|s,a_1) \le
    \varepsilon$ with probability $1-\varphi$, where $\bar T$ is the estimated transition function and $T$ is the true
    transition function -- then the model is considered to be correct.

    Based on the needed accuracy and the relation between $T$ and $\policy{2}$,
    \[
        P(s'|s,a_1) = T(s'|s,(a_1,a_2))\pi_2(a_2|s),
    \]
    we can show:
    \begin{align*}
        \bar P(s'|s,a_1)  -P(s'|s,a_1)
        & = T(s'|s,(a_1,a_2))\left( \pi_2(a_2|s) - \bar \pi_2(a_2|s;\paramVec^{(b)})
        \right)\\
        & \approx T(s'|s,(a_1,a_2)) \\
        & \quad \cdot \Big(\exp(Q(s,a_2)-V(s)/\kappa) \\
        & \qquad\ - \exp(Q(s,a_2;\paramVec^{(b)})-V(s;\paramVec^{(b)})/\kappa) \Big)
    \end{align*}

    Since $Q(s,a_2;\paramVec)$ is a Gaussian, $\exp Q(s,a_2;\paramVec)$ is log-normal. Remember that to the first two
    moments, the sum of lognormal random variables can be approximated by a lognormal random variable
    \cite{fenton1960sum}.  The approximated value function is the log-summation of log-normal random variables: ${V
    =\kappa \log \sum \exp Q(s,a;\paramVec)/\kappa}$. This can be approximated as a Gaussian. Therefore, we can quantify
    the bounded error between the estimated transition and true transition model, $\pi_2(a_2|s) - \bar
    \pi_2(a_2|s;\paramVec^{(b)})$,  by directly analyzing the distribution of the log-normal random variable
    $\exp(Q(s,a_2;\paramVec^{(b)})-V(s;\paramVec^{(b)}))$.

    We will use the linear combination of parameter-variances to quantify the variance of the Q-function.
    \begin{equation}\label{eq:state_action_uncertainty}
        \Omega(s,a_2)=\sum_{w=1 \in \Theta_{\unknown}} \nu_w^2 \phi_w^2(s,a_2).
    \end{equation}
    This can now be used as a metric for \agent{1} to either query, or explore, for the most informative data-batch
    $D^{(b)}$.

    \todo[inline]{If the random variable $\pi_2$ is \emph{log}-normal, shouldn't I be using the variance of the
                  log-normal distribution to reason about weather a state is known or not?
                  \[
                    X(s,a_2) = \left[\exp(\Omega(s,a_2) - 1)\right] \exp(2 Q(s,a_2) + \Omega(s,a_2)^2)
                  \]
                  I have been using Eq. \ref{eq:state_action_uncertainty} to guide the experiments.... }


